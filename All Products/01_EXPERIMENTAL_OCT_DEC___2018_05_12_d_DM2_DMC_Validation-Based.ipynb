{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DM2 DMC | All Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Building on datamining2/neuralnetworks/mlp_baseline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install XGBoost using e.g.: conda install -c rdonnelly py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introductory example on XGBoost, see: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = 'C:/Users/JulianWeller/Desktop/DM2_DMC_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_directory = 'C:/Users/JulianWeller/OneDrive - Julian Weller/01_MMDS/03_Semester/04_A_6_Data Mining II/03_DMC/02_Test_Data/DMC_2018_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import multiprocessing as mp\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of logical processors for speeding-up computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As provided by Chung (and modified to also filter Y_full):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "X_big = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "Y_big = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup with cluster filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2907\n",
      "(357561, 108)\n",
      "(357561, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Import cluster identifier\n",
    "# sales = pd.read_csv(working_directory + 'data_v0.1_sales.csv')\n",
    "# big_key = sales['key'][sales['cluster'] == \"big\"]\n",
    "# print(len(big_key.unique())) # Should only have 2907 keys remaining\n",
    "\n",
    "# # Import datasets\n",
    "# X_full = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "# Y_full = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))\n",
    "\n",
    "# # Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "# X_full['key'] = X_full['key'].astype(str)\n",
    "# X_big = X_full[X_full['key'].isin(big_key.astype(str))]\n",
    "# X_big = X_big.reset_index(drop=True)\n",
    "# print(X_big.shape) # Check the number of rows = 357,561\n",
    "\n",
    "# # Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "# Y_full['key'] = Y_full['key'].astype(str)\n",
    "# Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))]\n",
    "# Y_big = Y_big.reset_index(drop=True)\n",
    "# print(Y_big.shape) # Check the number of rows = 357,561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full = Y_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357561, 108)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357561, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full['month'] = pd.DatetimeIndex(X_full['date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full['month'] = pd.DatetimeIndex(Y_full['date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train = X_full.loc[X_full['month'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full_train = Y_full.loc[Y_full['month'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_test = X_full.loc[X_full['month'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full_test = Y_full.loc[Y_full['month'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Equal Step Width Leave-One-Out-Validation w.r.t. Dates with Lagged Embargo for Hyperparameter Tuning (Model Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When I use the term 'test set' in the context of validation, I refer to a subset of the training data, not to the January test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not have a lot of observations (Oct-Dec for training, only), it makes sense to use leave-one-out-validation w.r.t the date attribute. This also ensures that in our respective test sets, there are no overlapping observations from the training data w.r.t. to earliest and latest date of the test records. Consequently, \"purging\" as described by Lopez de Prado [2018] is not necessary. However, we have to prevent leakage from the respective training set into the respective test set by removing from the respective training set all records which dates \"[...] immediately follow an observation in the testing set. I call this process \"embargo.\"\" [Lopez de Prado, 2018]. As we only have one test date for validation in each round of the leave-one-out-validation, we simply have to remove all records from the respectively upcoming n days, where n is equal to the number of lags of sales data that we include as features times two(!). Times two indicates two different problems that need to be addressed: [1] \"embargo\" and [2] \"lagged embargo\" hereinafter. [1] For example, if we chose November 1 as one of the single validation dates, we would have to remove all records from the training set which date value is somewhere between (border values included) November 2 and November 15 (assume, that we drop 'last_15_day_sales', ..., 'last_28_day_sales' so that we do not loose too many training records). This is the above-mentioned \"embargo\". [2] As we deal with lagged features, we additionally have to remove all records that contain as values for the lagged features (sales) values from the \"embargo\" period. Consequently, following the example, we would have to remove all records with date values between (border values included) November 16 and November 29. E.g. the problem with November 29 is that it includes as lagged feature for all items 'last_14_day_sales' which refers in this case to November 15. The sales on November 15, however, have as lagged feature 'last_14_day_sales', as well. Unfortunately, this would refer here to November 1, which is our test date. To get rid of all undue leakage from test data into training data, however, we would have to remove records from November 29, as well, as we do not want to include a date in the training data which lagged feature value ('last_14_day_sales') is derived from data from the \"embargo\" period. One could argue that e.g. for November 29 data, we could at least keep 'last_1_day_sales', ..., 'last_13_day_sales'. That is certainly right, but would introduce a new problem: How to deal with the missing values (e.g. feature value 'last_14_day_sales' that is missing for November 29)? Thus, it might be reasonable to just drop records from all dates from (border values included) November 2 to November 29 in the example. As we have 2907 items in our \"big\" cluster, there are 2907 records for testing in each round of validation that are available, which should be sufficient. Due to the \"embargo\", we drop 14 x 2907 = 40,698 observations. Due to the \"lagged embargo\", we additionally have to drop the same number of observations. Consequently, we are left with (92-2x14) x 2907 = 186,048 records for training (that's about 52% of the complete training data set's records: 357561). We choose the testing dates such that they are equally-distributed: day 11, day 29, day 47, day 65, and day 83 (note the step-width of 18 and that there are 10 days before day 11 and 9 days after day 83 (until the last day in the training data, day 92)). That we train on the future to validate (test) on the past should not be an issue, as we assume by training on Oct-Dec and then finally testing on January data that the overall relationship between the features and the target remains the same and we make sure to remove all undue influence of the next 28 days after the respective validation dates, anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on why k-fold cross-validation might be problematic (thanks @Sun Jing for asking that important question): The problem with k-fold w.r.t items is that we cannot e.g. use the sales of item 2 on Nov 2 for training when we test on item 1 on Nov 1, as the sales of item 2 on Nov 2 are probably related to the sales of item 1 on Nov 1 (that is why Nov 2 is in the \"embargo\" time frame). W.r.t dates the problem is that we have too less data overall, as we also loose even more date due to the (lagged) embargo and that we have to avoid that we have to apply \"purging\" (cp. above) as this would further reduce the amount of training date available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://books.google.de/books?id=oU9KDwAAQBAJ&pg=PA103&lpg=PA110&dq=purged+cv+github&source=bl&ots=7TFGU-xxfx&sig=e94OZffPDeAaRJdn9k_pUHuR2t0&hl=de&sa=X&ved=0ahUKEwiNn-jXv6_aAhWFJZoKHWQCAOUQ6AEIXjAH#v=onepage&q&f=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each round of the validation, the respective validation_dates, embargo_dates and lagged_embargo_dates have to be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dates = [['2017-10-11'],\n",
    "                    ['2017-10-29'],\n",
    "                    ['2017-11-16'],\n",
    "                    ['2017-12-04'],\n",
    "                    ['2017-12-22']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embargo_dates = [['2017-10-12', '2017-10-13', '2017-10-14', '2017-10-15', '2017-10-16', '2017-10-17', '2017-10-18', '2017-10-19', '2017-10-20', '2017-10-21', '2017-10-22', '2017-10-23', '2017-10-24', '2017-10-25'],\n",
    "                 ['2017-10-30', '2017-10-31', '2017-11-01', '2017-11-02', '2017-11-03', '2017-11-04', '2017-11-05', '2017-11-06', '2017-11-07', '2017-11-08', '2017-11-09', '2017-11-10', '2017-11-11', '2017-11-12'],\n",
    "                 ['2017-11-17', '2017-11-18', '2017-11-19', '2017-11-20', '2017-11-21', '2017-11-22', '2017-11-23', '2017-11-24', '2017-11-25', '2017-11-26', '2017-11-27', '2017-11-28', '2017-11-29', '2017-11-30'],\n",
    "                 ['2017-12-05', '2017-12-06', '2017-12-07', '2017-12-08', '2017-12-09', '2017-12-10', '2017-12-11', '2017-12-12', '2017-12-13', '2017-12-14', '2017-12-15', '2017-12-16', '2017-12-17', '2017-12-18'],\n",
    "                 ['2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30', '2017-12-31']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_embargo_dates = [['2017-10-26', '2017-10-27', '2017-10-28', '2017-10-29', '2017-10-30', '2017-10-31', '2017-11-01', '2017-11-02', '2017-11-03', '2017-11-04', '2017-11-05', '2017-11-06', '2017-11-07', '2017-11-08'],\n",
    "                        ['2017-11-13', '2017-11-14', '2017-11-15', '2017-11-16', '2017-11-17', '2017-11-18', '2017-11-19', '2017-11-20', '2017-11-21', '2017-11-22', '2017-11-23', '2017-11-24', '2017-11-25', '2017-11-26'],\n",
    "                        ['2017-12-01', '2017-12-02', '2017-12-03', '2017-12-04', '2017-12-05', '2017-12-06', '2017-12-07', '2017-12-08', '2017-12-09', '2017-12-10', '2017-12-11', '2017-12-12', '2017-12-13', '2017-12-14'],\n",
    "                        ['2017-12-19', '2017-12-20', '2017-12-21', '2017-12-22', '2017-12-23', '2017-12-24', '2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30', '2017-12-31'],\n",
    "                        []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_cols = ['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'date', 'day_of_week', \n",
    "               'mainCategory', 'category', 'subCategory', 'releaseDate', \n",
    "               'rrp', 'price', 'month',\n",
    "               'last_15_day_sales', 'last_16_day_sales', 'last_17_day_sales', 'last_18_day_sales', 'last_19_day_sales', 'last_20_day_sales', 'last_21_day_sales', \n",
    "               'last_22_day_sales', 'last_23_day_sales', 'last_24_day_sales', 'last_25_day_sales', 'last_26_day_sales', 'last_27_day_sales', 'last_28_day_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_Y_cols = ['key', 'date', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    full_embargo_set = set(validation_dates[i] + embargo_dates[i] + lagged_embargo_dates[i])\n",
    "    validation_date = validation_dates[i]\n",
    "    \n",
    "    X_train_subsets.append(X_full_train.loc[X_full_train['date'].apply(lambda x: x not in full_embargo_set)].drop(drop_X_cols, axis=1).as_matrix())\n",
    "    Y_train_subsets.append(Y_full_train.loc[Y_full_train['date'].apply(lambda x: x not in full_embargo_set)].drop(drop_Y_cols, axis=1).as_matrix())\n",
    "\n",
    "    X_validation_subsets.append(X_full_train.loc[X_full_train['date'].apply(lambda x: x in validation_date)].drop(drop_X_cols, axis=1).as_matrix())\n",
    "    Y_validation_subsets.append(Y_full_train.loc[Y_full_train['date'].apply(lambda x: x in validation_date)].drop(drop_Y_cols, axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dates = pd.DataFrame(X_full['key']).join(X_full['date']) # Store for future lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_full_train.drop(drop_X_cols, axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_full_train.drop(drop_Y_cols, axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Lasso, LinearRegression, XGBRegressor, MLPRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_called = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_hyperparameters_options = {\n",
    "    'fit_intercept': [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*Lasso_hyperparameters_options.items())\n",
    "Lasso_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Hyperparameters to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression_hyperparameters_options = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'n_jobs': [cpus],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*LinearRegression_hyperparameters_options.items())\n",
    "LinearRegression_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_gbtree_options = {\n",
    "    'booster': ['gbtree'],\n",
    "    'n_jobs': [cpus],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [40, 65, 100],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'max_depth ': [5, 10, 15, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_gbtree_options.items())\n",
    "XGBoost_hyperparameters_gbtree = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_gblinear_options = {\n",
    "    'booster': ['gblinear'],\n",
    "    'n_jobs': [cpus],\n",
    "    #'learning_rate': [i/100 for i in range(21, 40, 1)],\n",
    "    #'n_estimators': [i for i in range(20, 120, 20)],\n",
    "    #'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    #'lambda': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_gblinear_options.items())\n",
    "XGBoost_hyperparameters_gblinear = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_dart_options = {\n",
    "    'booster': ['dart'],\n",
    "    'n_jobs': [cpus],\n",
    "    #'learning_rate': [i/100 for i in range(21, 40, 1)],\n",
    "    #'n_estimators': [i for i in range(40, 71, 1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_dart_options.items())\n",
    "XGBoost_hyperparameters_dart = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters = XGBoost_hyperparameters_gbtree + XGBoost_hyperparameters_gblinear + XGBoost_hyperparameters_dart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPRegressor Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPRegressor_hyperparameters_options = {\n",
    "    'activation': ['identity'],\n",
    "    #'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(25, )],\n",
    "    #'batch_size': ['auto', 10, 20, 40, 60, 80, 100],\n",
    "    #'max_iter': [10, 50, 100, 200],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*MLPRegressor_hyperparameters_options.items())\n",
    "MLPRegressor_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Settings to Try for all Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameters = [Lasso_hyperparameters,\n",
    "                         LinearRegression_hyperparameters,\n",
    "                         XGBoost_hyperparameters,\n",
    "                         MLPRegressor_hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_avg_rmse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id, model_val in enumerate(models):\n",
    "    model_hyperparameters_called = []\n",
    "    models_hyperparameters_avg_rmse_scores = []\n",
    "    \n",
    "    for model_hyperparameter_id, model_hyperparameter_val in enumerate(model_hyperparameters[model_id]):\n",
    "        model = model_val(**model_hyperparameter_val)\n",
    "        model_hyperparameters_called.append(model)\n",
    "        models_hyperparameters_rmse_score_subset = []\n",
    "        \n",
    "        for X_train_subset_id, X_train_subset_val in enumerate(X_train_subsets):\n",
    "            model.fit(X_train_subset_val, Y_train_subsets[X_train_subset_id])\n",
    "            models_hyperparameters_rmse_score_subset.append(sqrt(mean_squared_error(Y_validation_subsets[X_train_subset_id], np.round(model.predict(X_validation_subsets[X_train_subset_id])))))\n",
    "            \n",
    "        models_hyperparameters_avg_rmse_scores.append(np.average(models_hyperparameters_rmse_score_subset))\n",
    "\n",
    "        print(model_hyperparameters_called[-1:])\n",
    "        print(models_hyperparameters_avg_rmse_scores[-1:])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    models_called.append(model_hyperparameters_called)\n",
    "    models_avg_rmse_scores.append(models_hyperparameters_avg_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter settings for the respective models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for models_avg_rmse_scores_id, models_avg_rmse_scores_val in enumerate(models_avg_rmse_scores):\n",
    "    selected_model = models_called[models_avg_rmse_scores_id][models_avg_rmse_scores_val.index(min(models_avg_rmse_scores_val))]\n",
    "    print(selected_model)\n",
    "    selected_models.append(selected_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models= [Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
    "   normalize=False, positive=False, precompute=False, random_state=None,\n",
    "   selection='cyclic', tol=0.0001, warm_start=False),\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False),\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
    "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.7),\n",
    "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(25,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=None,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False),\n",
       " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "        max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
       "        n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "        silent=True, subsample=0.7),\n",
       " MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
       "        beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "        hidden_layer_sizes=(25,), learning_rate='constant',\n",
       "        learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "        nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "        shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "        verbose=False, warm_start=False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks Lu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### 1. Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Import cluster identifier\n",
    "\n",
    "# sales = pd.read_csv(working_directory + 'data_v0.1_sales.csv')\n",
    "# big_key = sales['key'][sales['cluster'] == \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Import datasets\n",
    "\n",
    "# X_full = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "# Y_full = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_cols = X_full.columns[12+14:12+14+14]\n",
    "#X_full = X_full.drop(drop_cols, axis=1)\n",
    "#print(X_full.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Find rows which belong to cluster 'big' in X_full\n",
    "\n",
    "# X_full['key'] = X_full['key'].astype(str)\n",
    "# X_big = X_full[X_full['key'].isin(big_key.astype(str))]\n",
    "# X_big = X_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Find rows which belong to cluster 'big' in Y_full\n",
    "\n",
    "# Y_full['key'] = Y_full['key'].astype(str)\n",
    "# Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))]\n",
    "# Y_big = Y_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the X_big and Y_big into traing and test\n",
    "\n",
    "X_big['month'] = pd.DatetimeIndex(X_big['date']).month\n",
    "Y_big['month'] = pd.DatetimeIndex(Y_big['date']).month\n",
    "X_big_train = X_big.loc[X_big['month'] != 1]\n",
    "Y_train = Y_big.loc[Y_big['month'] != 1]['sales']\n",
    "X_big_test = X_big.loc[X_big['month'] == 1]\n",
    "Y_test = Y_big.loc[Y_big['month'] == 1]['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['last_1_day_sales', 'last_2_day_sales', 'last_3_day_sales',\n",
      "       'last_4_day_sales', 'last_5_day_sales', 'last_6_day_sales',\n",
      "       'last_7_day_sales', 'last_8_day_sales', 'last_9_day_sales',\n",
      "       'last_10_day_sales', 'last_11_day_sales', 'last_12_day_sales',\n",
      "       'last_13_day_sales', 'last_14_day_sales', 'last_15_day_sales',\n",
      "       'last_16_day_sales', 'last_17_day_sales', 'last_18_day_sales',\n",
      "       'last_19_day_sales', 'last_20_day_sales', 'last_21_day_sales',\n",
      "       'last_22_day_sales', 'last_23_day_sales', 'last_24_day_sales',\n",
      "       'last_25_day_sales', 'last_26_day_sales', 'last_27_day_sales',\n",
      "       'last_28_day_sales', 'is_eleventh', 'is_crazy_day', 'day_Friday',\n",
      "       'day_Monday', 'day_Saturday', 'day_Sunday', 'day_Thursday',\n",
      "       'day_Tuesday', 'day_Wednesday', 'days_since_release', 'price_diff',\n",
      "       'color_beige', 'color_blau', 'color_braun', 'color_gelb', 'color_gold',\n",
      "       'color_grau', 'color_gruen', 'color_khaki', 'color_lila',\n",
      "       'color_orange', 'color_pink', 'color_rosa', 'color_rot',\n",
      "       'color_schwarz', 'color_silber', 'color_tuerkis', 'color_weiss',\n",
      "       'brand_Asics', 'brand_Cinquestelle', 'brand_Converse', 'brand_Diadora',\n",
      "       'brand_Erima', 'brand_FREAM', 'brand_Hummel', 'brand_Jako',\n",
      "       'brand_Jordan', 'brand_KangaROOS', 'brand_Kempa', 'brand_Lotto',\n",
      "       'brand_Mizuno', 'brand_New Balance', 'brand_Nike', 'brand_Onitsuka',\n",
      "       'brand_PUMA', 'brand_Reebok', 'brand_Reusch', 'brand_Sells',\n",
      "       'brand_Sport2000', 'brand_Stance', 'brand_Uhlsport',\n",
      "       'brand_Under Armour', 'brand_adidas', 'maincat_1', 'maincat_9',\n",
      "       'maincat_15', 'cat_2', 'cat_7', 'cat_10', 'cat_16', 'cat_18', 'cat_24',\n",
      "       'cat_30', 'cat_33', 'cat_36', 'cat_37', 'marketing_activity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for fitting the input of the model\n",
    "\n",
    "drop_x_cols = ['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'date', 'day_of_week', \n",
    "               'mainCategory', 'category', 'subCategory', 'releaseDate', \n",
    "               'rrp', 'price', 'month']\n",
    "X_train = X_big_train.drop(drop_x_cols, axis=1)\n",
    "X_test = X_big_test.drop(drop_x_cols, axis=1)\n",
    "print(X_train.columns)\n",
    "X_train = X_train.as_matrix()\n",
    "X_train = np.delete(X_train, np.s_[14:28], axis=1)\n",
    "Y_train = Y_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "X_test = np.delete(X_test, np.s_[14:28], axis=1)\n",
    "Y_test = Y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397544, 81)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
       "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.7)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "model = selected_models[2]\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# Only the sale unit of the first day for each item is right int the 'X_test'\n",
    "# Select the row 'on Jan 1st'\n",
    "\n",
    "X_Jan1 = X_test[0:1,:]\n",
    "for i in range(int(len(X_test)/31-1)):\n",
    "    X_Jan1 = np.vstack([X_Jan1, X_test[(31+i*31):(32+i*31),:]])\n",
    "print(X_Jan1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12824, 81)\n"
     ]
    }
   ],
   "source": [
    "print(X_Jan1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the sales unit 'on Jan 1st' for each items\n",
    "# Change the format of the prediction results on Jan_1st\n",
    "\n",
    "Y_Jan1 = model.predict(X_Jan1)\n",
    "prediction_1 = np.asarray([round(value) for value in Y_Jan1])\n",
    "prediction_1 = np.reshape(prediction_1, (len(prediction_1),1))\n",
    "print(prediction_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Delete the 'last_28_day_sales'\n",
    "# Add the prediction results as the 'last_1_day_sales'\n",
    "\n",
    "X_Jan = X_Jan1\n",
    "X_Jan = np.delete(X_Jan, np.s_[13:14], axis=1)\n",
    "X_Jan = np.append(prediction_1, X_Jan, axis=1)\n",
    "print(X_Jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Add the process above into a loop\n",
    "# Predict the sales units eery day in January for each item\n",
    "\n",
    "predictions = prediction_1\n",
    "for i in range(30):\n",
    "    Y_Jan = model.predict(X_Jan)\n",
    "    prediction = np.asarray([round(value) for value in Y_Jan])\n",
    "    prediction = np.reshape(prediction, (len(prediction),1))\n",
    "    predictions = np.append(predictions, prediction, axis=1)\n",
    "    X_Jan = np.delete(X_Jan, np.s_[13:14], axis=1)\n",
    "    X_Jan = np.append(prediction, X_Jan, axis=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Restructure the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reshape predictions: row ->'big' items, columns -> date\n",
    "\n",
    "column_name = X_big_test['date'].unique().astype(str)\n",
    "row_name = X_big_test['key'].unique().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Aggregate sales for each day each item.\n",
    "\n",
    "pred_agg = predictions\n",
    "agg_sum = predictions[:,0]\n",
    "for i in range(len(column_name)-1):\n",
    "    agg_sum = pred_agg[:,i] + predictions[:, i+1]\n",
    "    pred_agg[:, i+1] = agg_sum\n",
    "print(pred_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test_0.csv\n",
    "#### 2.1. Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "\n",
    "test_0 = pd.read_csv(test_data_directory + 'test_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10001L' 1.0 '2018-01-07']\n",
      " ['100035 ( 43-46 )' 1.0 '2018-01-30']\n",
      " ['10008XL' 4.0 '2018-01-30']\n",
      " ...\n",
      " ['228782XL' 1.0 '2018-01-02']\n",
      " ['22878M' 2.0 '2018-01-28']\n",
      " ['22881S' 1.0 '2018-01-02']]\n"
     ]
    }
   ],
   "source": [
    "# Add 'key' for test data by merging 'pid' and 'size'\n",
    "# Select useful attributes\n",
    "\n",
    "test_0[\"key\"] = test_0[\"pid\"].map(int).map(str) + test_0[\"size\"]\n",
    "test_0_big = test_0.loc[test_0['key'].isin(row_name)]\n",
    "subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "test = np.asarray(subtest_0_big)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Match the 'test' with the 'pred_agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arrays for storing the predicted day and date\n",
    "\n",
    "pred_day = np.zeros((len(test),1), dtype=int)\n",
    "pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # 'key' is the key for each test item\n",
    "    key = test[i,0]\n",
    "    # Find the index of the item in predictions sharing the same key\n",
    "    index = 0\n",
    "    \n",
    "    # Match the items in 'test' with the items in 'pred_agg'\n",
    "    # Retern the index of the item in the 'pred_agg'\n",
    "    for j in range(len(row_name)):\n",
    "        if row_name[j] == key:\n",
    "            index = j\n",
    "            break\n",
    "    \n",
    "    # Match\n",
    "    if test[i,1] < pred_agg[index,0]:\n",
    "        pred_day[i,0] = 1\n",
    "        pred_date[i,0] = column_name[0]\n",
    "        continue\n",
    "    if test[i,1] > pred_agg[index,30]:\n",
    "        pred_day[i,0] = 15\n",
    "        pred_date[i,0] = column_name[14]\n",
    "        continue\n",
    "    for k in range(len(pred_agg[0])):\n",
    "        if pred_agg[index,k] - test[i,1] >=0:\n",
    "            pred_day[i,0] = k+1\n",
    "            pred_date[i,0] = column_name[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Error Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7]\n",
      " [30]\n",
      " [30]\n",
      " ...\n",
      " [ 2]\n",
      " [28]\n",
      " [ 2]]\n"
     ]
    }
   ],
   "source": [
    "# Find the day of the real sold out date\n",
    "\n",
    "day = [i[-2:] for i in test[:,2].tolist()]\n",
    "real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "print(real_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15]\n",
      " [15]\n",
      " [15]\n",
      " ...\n",
      " [15]\n",
      " [15]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "# Show the predicted sold out date\n",
    "\n",
    "print(pred_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.20921749620015\n"
     ]
    }
   ],
   "source": [
    "# Error calculation\n",
    "\n",
    "error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>stock</th>\n",
       "      <th>real_sold_out_day</th>\n",
       "      <th>predicted_sold_out_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100035 ( 43-46 )</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10008XL</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10013L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10013M</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10020XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1003143</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10035L ( 152-158 )</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10035XL ( 158-170 )</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10035XS ( 116-128 )</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1003946</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10043L</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10043XL</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10046L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10046M</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1004842</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10051XL ( 158-170 )</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10061L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10062L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10063L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10063M</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10063XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1006444</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1006444 2/3</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100652 ( 37-39 )</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100654 ( 43-45 )</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1006935</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1006936</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-15</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1006937 1/3</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-10</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10073L ( 40/42 )</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>228232XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-22</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>22823L</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8114</th>\n",
       "      <td>22823M</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8115</th>\n",
       "      <td>22823XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>22835L</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8117</th>\n",
       "      <td>22835XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>228422XL</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>22842L</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>22842M</td>\n",
       "      <td>28</td>\n",
       "      <td>2018-01-13</td>\n",
       "      <td>2018-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>22842S</td>\n",
       "      <td>18</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>22842XL</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8123</th>\n",
       "      <td>22842XS</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8124</th>\n",
       "      <td>2284438</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>2284443</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>2285239</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8127</th>\n",
       "      <td>22861XL ( 158-170 )</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-24</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8128</th>\n",
       "      <td>2286340</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8129</th>\n",
       "      <td>22864M</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-18</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>2286642</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8131</th>\n",
       "      <td>2286939</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>2286940</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8133</th>\n",
       "      <td>2286941</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8134</th>\n",
       "      <td>2286942</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8135</th>\n",
       "      <td>2286943</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8136</th>\n",
       "      <td>2286944</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td>2286945</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>22872M ( 140-152 )</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>228782XL</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>22878M</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8141</th>\n",
       "      <td>22881S</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8142 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key stock real_sold_out_day predicted_sold_out_day\n",
       "0                  10001L     1        2018-01-07             2018-01-15\n",
       "1        100035 ( 43-46 )     1        2018-01-30             2018-01-15\n",
       "2                 10008XL     4        2018-01-30             2018-01-15\n",
       "3                  10013L     1        2018-01-24             2018-01-15\n",
       "4                  10013M     1        2018-01-22             2018-01-15\n",
       "5                 10020XL     1        2018-01-18             2018-01-15\n",
       "6                 1003143     1        2018-01-11             2018-01-15\n",
       "7      10035L ( 152-158 )     1        2018-01-26             2018-01-15\n",
       "8     10035XL ( 158-170 )     4        2018-01-16             2018-01-15\n",
       "9     10035XS ( 116-128 )     1        2018-01-31             2018-01-15\n",
       "10                1003946     1        2018-01-01             2018-01-15\n",
       "11                 10043L     2        2018-01-31             2018-01-15\n",
       "12                10043XL     2        2018-01-30             2018-01-15\n",
       "13                 10046L     1        2018-01-31             2018-01-15\n",
       "14                 10046M     1        2018-01-15             2018-01-15\n",
       "15                1004842     1        2018-01-10             2018-01-15\n",
       "16    10051XL ( 158-170 )     1        2018-01-02             2018-01-15\n",
       "17                 10061L     1        2018-01-05             2018-01-15\n",
       "18                 10062L     1        2018-01-01             2018-01-15\n",
       "19                 10063L     1        2018-01-12             2018-01-15\n",
       "20                 10063M     8        2018-01-26             2018-01-15\n",
       "21                10063XL     1        2018-01-03             2018-01-15\n",
       "22                1006444     1        2018-01-22             2018-01-15\n",
       "23            1006444 2/3     1        2018-01-25             2018-01-15\n",
       "24       100652 ( 37-39 )    20        2018-01-24             2018-01-15\n",
       "25       100654 ( 43-45 )     3        2018-01-18             2018-01-15\n",
       "26                1006935     1        2018-01-10             2018-01-15\n",
       "27                1006936     4        2018-01-15             2018-01-15\n",
       "28            1006937 1/3     1        2018-01-10             2018-01-15\n",
       "29       10073L ( 40/42 )     9        2018-01-04             2018-01-15\n",
       "...                   ...   ...               ...                    ...\n",
       "8112             228232XL     1        2018-01-22             2018-01-15\n",
       "8113               22823L     1        2018-01-12             2018-01-15\n",
       "8114               22823M     2        2018-01-17             2018-01-15\n",
       "8115              22823XL     1        2018-01-29             2018-01-15\n",
       "8116               22835L     2        2018-01-19             2018-01-15\n",
       "8117              22835XL     1        2018-01-16             2018-01-15\n",
       "8118             228422XL    11        2018-01-18             2018-01-15\n",
       "8119               22842L   220        2018-01-28             2018-01-15\n",
       "8120               22842M    28        2018-01-13             2018-01-13\n",
       "8121               22842S    18        2018-01-31             2018-01-15\n",
       "8122              22842XL    10        2018-01-08             2018-01-15\n",
       "8123              22842XS     4        2018-01-29             2018-01-15\n",
       "8124              2284438     1        2018-01-09             2018-01-15\n",
       "8125              2284443     1        2018-01-29             2018-01-15\n",
       "8126              2285239     1        2018-01-31             2018-01-15\n",
       "8127  22861XL ( 158-170 )     2        2018-01-24             2018-01-15\n",
       "8128              2286340     1        2018-01-04             2018-01-15\n",
       "8129               22864M     1        2018-01-18             2018-01-15\n",
       "8130              2286642     1        2018-01-07             2018-01-15\n",
       "8131              2286939     1        2018-01-26             2018-01-15\n",
       "8132              2286940     1        2018-01-04             2018-01-15\n",
       "8133              2286941     5        2018-01-31             2018-01-15\n",
       "8134              2286942     1        2018-01-31             2018-01-15\n",
       "8135              2286943     3        2018-01-31             2018-01-15\n",
       "8136              2286944     2        2018-01-31             2018-01-15\n",
       "8137              2286945     2        2018-01-25             2018-01-15\n",
       "8138   22872M ( 140-152 )     4        2018-01-25             2018-01-15\n",
       "8139             228782XL     1        2018-01-02             2018-01-15\n",
       "8140               22878M     2        2018-01-28             2018-01-15\n",
       "8141               22881S     1        2018-01-02             2018-01-15\n",
       "\n",
       "[8142 rows x 4 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the result in dataframe\n",
    "\n",
    "result = np.append(test, pred_date, axis=1)\n",
    "result_column =['key','stock','real_sold_out_day','predicted_sold_out_day']\n",
    "prediction_result = pd.DataFrame(result, columns=result_column)\n",
    "prediction_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_0 = pd.read_csv(test_data_directory + 'test_1.csv')\n",
    "\n",
    "# Add 'key' for test data by merging 'pid' and 'size'\n",
    "# Select useful attributes\n",
    "test_0[\"key\"] = test_0[\"pid\"].map(int).map(str) + test_0[\"size\"]\n",
    "test_0_big = test_0.loc[test_0['key'].isin(row_name)]\n",
    "subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "test = np.asarray(subtest_0_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arrays for storing the predicted day and date\n",
    "\n",
    "pred_day = np.zeros((len(test),1), dtype=int)\n",
    "pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # 'key' is the key for each test item\n",
    "    key = test[i,0]\n",
    "    # Find the index of the item in predictions sharing the same key\n",
    "    index = 0\n",
    "    \n",
    "    # Match the items in 'test' with the items in 'pred_agg'\n",
    "    # Retern the index of the item in the 'pred_agg'\n",
    "    for j in range(len(row_name)):\n",
    "        if row_name[j] == key:\n",
    "            index = j\n",
    "            break\n",
    "    \n",
    "    # Match\n",
    "    if test[i,1] < pred_agg[index,0]:\n",
    "        pred_day[i,0] = 1\n",
    "        pred_date[i,0] = column_name[0]\n",
    "        continue\n",
    "    if test[i,1] > pred_agg[index,30]:\n",
    "        pred_day[i,0] = 15\n",
    "        pred_date[i,0] = column_name[14]\n",
    "        continue\n",
    "    for k in range(len(pred_agg[0])):\n",
    "        if pred_agg[index,k] - test[i,1] >=0:\n",
    "            pred_day[i,0] = k+1\n",
    "            pred_date[i,0] = column_name[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.2034081881957\n"
     ]
    }
   ],
   "source": [
    "# Error calculation\n",
    "\n",
    "day = [i[-2:] for i in test[:,2].tolist()]\n",
    "real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_0 = pd.read_csv(test_data_directory + 'test_2.csv')\n",
    "\n",
    "# Add 'key' for test data by merging 'pid' and 'size'\n",
    "# Select useful attributes\n",
    "test_0[\"key\"] = test_0[\"pid\"].map(int).map(str) + test_0[\"size\"]\n",
    "test_0_big = test_0.loc[test_0['key'].isin(row_name)]\n",
    "subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "test = np.asarray(subtest_0_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arrays for storing the predicted day and date\n",
    "\n",
    "pred_day = np.zeros((len(test),1), dtype=int)\n",
    "pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # 'key' is the key for each test item\n",
    "    key = test[i,0]\n",
    "    # Find the index of the item in predictions sharing the same key\n",
    "    index = 0\n",
    "    \n",
    "    # Match the items in 'test' with the items in 'pred_agg'\n",
    "    # Retern the index of the item in the 'pred_agg'\n",
    "    for j in range(len(row_name)):\n",
    "        if row_name[j] == key:\n",
    "            index = j\n",
    "            break\n",
    "    \n",
    "    # Match\n",
    "    if test[i,1] < pred_agg[index,0]:\n",
    "        pred_day[i,0] = 1\n",
    "        pred_date[i,0] = column_name[0]\n",
    "        continue\n",
    "    if test[i,1] > pred_agg[index,30]:\n",
    "        pred_day[i,0] = 15\n",
    "        pred_date[i,0] = column_name[14]\n",
    "        continue\n",
    "    for k in range(len(pred_agg[0])):\n",
    "        if pred_agg[index,k] - test[i,1] >=0:\n",
    "            pred_day[i,0] = k+1\n",
    "            pred_date[i,0] = column_name[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258.5169240107889\n"
     ]
    }
   ],
   "source": [
    "# Error calculation\n",
    "\n",
    "day = [i[-2:] for i in test[:,2].tolist()]\n",
    "real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_0 = pd.read_csv(test_data_directory + 'test_3.csv')\n",
    "\n",
    "# Add 'key' for test data by merging 'pid' and 'size'\n",
    "# Select useful attributes\n",
    "test_0[\"key\"] = test_0[\"pid\"].map(int).map(str) + test_0[\"size\"]\n",
    "test_0_big = test_0.loc[test_0['key'].isin(row_name)]\n",
    "subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "test = np.asarray(subtest_0_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arrays for storing the predicted day and date\n",
    "\n",
    "pred_day = np.zeros((len(test),1), dtype=int)\n",
    "pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # 'key' is the key for each test item\n",
    "    key = test[i,0]\n",
    "    # Find the index of the item in predictions sharing the same key\n",
    "    index = 0\n",
    "    \n",
    "    # Match the items in 'test' with the items in 'pred_agg'\n",
    "    # Retern the index of the item in the 'pred_agg'\n",
    "    for j in range(len(row_name)):\n",
    "        if row_name[j] == key:\n",
    "            index = j\n",
    "            break\n",
    "    \n",
    "    # Match\n",
    "    if test[i,1] < pred_agg[index,0]:\n",
    "        pred_day[i,0] = 1\n",
    "        pred_date[i,0] = column_name[0]\n",
    "        continue\n",
    "    if test[i,1] > pred_agg[index,30]:\n",
    "        pred_day[i,0] = 15\n",
    "        pred_date[i,0] = column_name[14]\n",
    "        continue\n",
    "    for k in range(len(pred_agg[0])):\n",
    "        if pred_agg[index,k] - test[i,1] >=0:\n",
    "            pred_day[i,0] = k+1\n",
    "            pred_date[i,0] = column_name[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257.9961240018927\n"
     ]
    }
   ],
   "source": [
    "# Error calculation\n",
    "\n",
    "day = [i[-2:] for i in test[:,2].tolist()]\n",
    "real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_0 = pd.read_csv(test_data_directory + 'test_4.csv')\n",
    "\n",
    "# Add 'key' for test data by merging 'pid' and 'size'\n",
    "# Select useful attributes\n",
    "test_0[\"key\"] = test_0[\"pid\"].map(int).map(str) + test_0[\"size\"]\n",
    "test_0_big = test_0.loc[test_0['key'].isin(row_name)]\n",
    "subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "test = np.asarray(subtest_0_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arrays for storing the predicted day and date\n",
    "\n",
    "pred_day = np.zeros((len(test),1), dtype=int)\n",
    "pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # 'key' is the key for each test item\n",
    "    key = test[i,0]\n",
    "    # Find the index of the item in predictions sharing the same key\n",
    "    index = 0\n",
    "    \n",
    "    # Match the items in 'test' with the items in 'pred_agg'\n",
    "    # Retern the index of the item in the 'pred_agg'\n",
    "    for j in range(len(row_name)):\n",
    "        if row_name[j] == key:\n",
    "            index = j\n",
    "            break\n",
    "    \n",
    "    # Match\n",
    "    if test[i,1] < pred_agg[index,0]:\n",
    "        pred_day[i,0] = 1\n",
    "        pred_date[i,0] = column_name[0]\n",
    "        continue\n",
    "    if test[i,1] > pred_agg[index,30]:\n",
    "        pred_day[i,0] = 15\n",
    "        pred_date[i,0] = column_name[14]\n",
    "        continue\n",
    "    for k in range(len(pred_agg[0])):\n",
    "        if pred_agg[index,k] - test[i,1] >=0:\n",
    "            pred_day[i,0] = k+1\n",
    "            pred_date[i,0] = column_name[k]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257.93603858321154\n"
     ]
    }
   ],
   "source": [
    "# Error calculation\n",
    "\n",
    "day = [i[-2:] for i in test[:,2].tolist()]\n",
    "real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
