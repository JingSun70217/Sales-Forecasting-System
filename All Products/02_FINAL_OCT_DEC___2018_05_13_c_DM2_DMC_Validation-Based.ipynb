{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DM2 DMC | Validation-based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Building on datamining2/neuralnetworks/mlp_baseline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install XGBoost using e.g.: conda install -c rdonnelly py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introductory example on XGBoost, see: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories and Dumping/Reading-In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = 'C:/Users/JulianWeller/Desktop/DM2_DMC_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_directory = 'C:/Users/JulianWeller/OneDrive - Julian Weller/01_MMDS/03_Semester/04_A_6_Data Mining II/03_DMC/02_Test_Data/DMC_2018_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_write_selected_models = 'w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import multiprocessing as mp\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of logical processors for speeding-up computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As provided by Chung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "Y_big = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup when using clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import cluster identifier\n",
    "# sales = pd.read_csv(working_directory + 'data_v0.1_sales.csv')\n",
    "# big_key = sales['key'][sales['cluster'] == \"big\"]\n",
    "# print(len(big_key.unique())) # Should only have 2907 keys remaining\n",
    "\n",
    "# # Import datasets\n",
    "# X_full = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "# Y_full = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))\n",
    "\n",
    "# # Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "# X_full['key'] = X_full['key'].astype(str)\n",
    "# X_big = X_full[X_full['key'].isin(big_key.astype(str))]\n",
    "# X_big = X_big.reset_index(drop=True)\n",
    "# print(X_big.shape) # Check the number of rows = 357,561\n",
    "\n",
    "# # Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "# Y_full['key'] = Y_full['key'].astype(str)\n",
    "# Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))]\n",
    "# Y_big = Y_big.reset_index(drop=True)\n",
    "# print(Y_big.shape) # Check the number of rows = 357,561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = X_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full = Y_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1577352, 108)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1577352, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full['month'] = pd.DatetimeIndex(X_full['date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full['month'] = pd.DatetimeIndex(Y_full['date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train = X_full.loc[X_full['month'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full_train = Y_full.loc[Y_full['month'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_test = X_full.loc[X_full['month'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full_test = Y_full.loc[Y_full['month'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Equal Step Width Leave-One-Out-Validation w.r.t. Dates with Lagged Embargo for Hyperparameter Tuning (Model Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Adjusted from 5 to 10 validation dates in the meantime (description is still based on 5 validation dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When I use the term 'test set' in the context of validation, I refer to a subset of the training data, not to the January test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not have a lot of observations (Oct-Dec for training, only), it makes sense to use leave-one-out-validation w.r.t the date attribute. This also ensures that in our respective test sets, there are no overlapping observations from the training data w.r.t. to earliest and latest date of the test records. Consequently, \"purging\" as described by Lopez de Prado [2018] is not necessary. However, we have to prevent leakage from the respective training set into the respective test set by removing from the respective training set all records which dates \"[...] immediately follow an observation in the testing set. I call this process \"embargo.\"\" [Lopez de Prado, 2018]. As we only have one test date for validation in each round of the leave-one-out-validation, we simply have to remove all records from the respectively upcoming n days, where n is equal to the number of lags of sales data that we include as features times two(!). Times two indicates two different problems that need to be addressed: [1] \"embargo\" and [2] \"lagged embargo\" hereinafter. [1] For example, if we chose November 1 as one of the single validation dates, we would have to remove all records from the training set which date value is somewhere between (border values included) November 2 and November 15 (assume, that we drop 'last_15_day_sales', ..., 'last_28_day_sales' so that we do not loose too many training records). This is the above-mentioned \"embargo\". [2] As we deal with lagged features, we additionally have to remove all records that contain as values for the lagged features (sales) values from the \"embargo\" period. Consequently, following the example, we would have to remove all records with date values between (border values included) November 16 and November 29. E.g. the problem with November 29 is that it includes as lagged feature for all items 'last_14_day_sales' which refers in this case to November 15. The sales on November 15, however, have as lagged feature 'last_14_day_sales', as well. Unfortunately, this would refer here to November 1, which is our test date. To get rid of all undue leakage from test data into training data, however, we would have to remove records from November 29, as well, as we do not want to include a date in the training data which lagged feature value ('last_14_day_sales') is derived from data from the \"embargo\" period. One could argue that e.g. for November 29 data, we could at least keep 'last_1_day_sales', ..., 'last_13_day_sales'. That is certainly right, but would introduce a new problem: How to deal with the missing values (e.g. feature value 'last_14_day_sales' that is missing for November 29)? Thus, it might be reasonable to just drop records from all dates from (border values included) November 2 to November 29 in the example. As we have 2907 items in our \"big\" cluster, there are 2907 records for testing in each round of validation that are available, which should be sufficient. Due to the \"embargo\", we drop 14 x 2907 = 40,698 observations. Due to the \"lagged embargo\", we additionally have to drop the same number of observations. Consequently, we are left with (92-2x14) x 2907 = 186,048 records for training (that's about 52% of the complete training data set's records: 357561). We choose the testing dates such that they are equally-distributed: day 11, day 29, day 47, day 65, and day 83 (note the step-width of 18 and that there are 10 days before day 11 and 9 days after day 83 (until the last day in the training data, day 92)). That we train on the future to validate (test) on the past should not be an issue, as we assume by training on Oct-Dec and then finally testing on January data that the overall relationship between the features and the target remains the same and we make sure to remove all undue influence of the next 28 days after the respective validation dates, anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on why k-fold cross-validation might be problematic (thanks @Sun Jing for asking that important question): The problem with k-fold w.r.t items is that we cannot e.g. use the sales of item 2 on Nov 2 for training when we test on item 1 on Nov 1, as the sales of item 2 on Nov 2 are probably related to the sales of item 1 on Nov 1 (that is why Nov 2 is in the \"embargo\" time frame). W.r.t dates the problem is that we have too less data overall, as we also loose even more date due to the (lagged) embargo and that we have to avoid that we have to apply \"purging\" (cp. above) as this would further reduce the amount of training date available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://books.google.de/books?id=oU9KDwAAQBAJ&pg=PA103&lpg=PA110&dq=purged+cv+github&source=bl&ots=7TFGU-xxfx&sig=e94OZffPDeAaRJdn9k_pUHuR2t0&hl=de&sa=X&ved=0ahUKEwiNn-jXv6_aAhWFJZoKHWQCAOUQ6AEIXjAH#v=onepage&q&f=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each round of the validation, the respective validation_dates, embargo_dates and lagged_embargo_dates have to be removed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 validation dates are equally distributed across the training data (we always add 92/10, that's why the number of days between the dates varies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dates = [['2017-10-05'],\n",
    "                    ['2017-10-14'],\n",
    "                    ['2017-10-24'],\n",
    "                    ['2017-11-02'],\n",
    "                    ['2017-11-11'],\n",
    "                    ['2017-11-20'],\n",
    "                    ['2017-11-29'],\n",
    "                    ['2017-12-09'],\n",
    "                    ['2017-12-18'],\n",
    "                    ['2017-12-27']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_possible_training_day = '2017-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embargo_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for validation_date in validation_dates:\n",
    "    candidate_embargo_dates = pd.date_range(validation_date[0], periods=15).strftime('%Y-%m-%d').tolist()[1:]\n",
    "    \n",
    "    final_embargo_dates = [i for i in candidate_embargo_dates if i <= last_possible_training_day]\n",
    "    \n",
    "    embargo_dates.append(final_embargo_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_embargo_dates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for validation_date in validation_dates:\n",
    "    \n",
    "    candidate_lagged_embargo_dates = pd.date_range(validation_date[0], periods=29).strftime('%Y-%m-%d').tolist()[15:]\n",
    "    \n",
    "    final_lagged_embargo_dates = [i for i in candidate_lagged_embargo_dates if i <= last_possible_training_day]\n",
    "    \n",
    "    lagged_embargo_dates.append(final_lagged_embargo_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_dates = [[validation_date[0], embargo_dates, lagged_embargo_dates] for validation_date, embargo_dates, lagged_embargo_dates in zip(validation_dates, embargo_dates, lagged_embargo_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Date</th>\n",
       "      <th>Embargo Dates</th>\n",
       "      <th>Lagged Embargo Dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-05</td>\n",
       "      <td>[2017-10-06, 2017-10-07, 2017-10-08, 2017-10-09, 2017-10-10, 2017-10-11, 2017-10-12, 2017-10-13, 2017-10-14, 2017-10-15, 2017-10-16, 2017-10-17, 2017-10-18, 2017-10-19]</td>\n",
       "      <td>[2017-10-20, 2017-10-21, 2017-10-22, 2017-10-23, 2017-10-24, 2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28, 2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>[2017-10-15, 2017-10-16, 2017-10-17, 2017-10-18, 2017-10-19, 2017-10-20, 2017-10-21, 2017-10-22, 2017-10-23, 2017-10-24, 2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28]</td>\n",
       "      <td>[2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02, 2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07, 2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>[2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28, 2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02, 2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07]</td>\n",
       "      <td>[2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11, 2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16, 2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>[2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07, 2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11, 2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16]</td>\n",
       "      <td>[2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25, 2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>[2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16, 2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25]</td>\n",
       "      <td>[2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04, 2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>[2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25, 2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04]</td>\n",
       "      <td>[2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09, 2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13, 2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>[2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04, 2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09, 2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13]</td>\n",
       "      <td>[2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18, 2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23, 2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>[2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13, 2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18, 2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23]</td>\n",
       "      <td>[2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27, 2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>[2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23, 2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27, 2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>[2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Validation Date  \\\n",
       "0  2017-10-05       \n",
       "1  2017-10-14       \n",
       "2  2017-10-24       \n",
       "3  2017-11-02       \n",
       "4  2017-11-11       \n",
       "5  2017-11-20       \n",
       "6  2017-11-29       \n",
       "7  2017-12-09       \n",
       "8  2017-12-18       \n",
       "9  2017-12-27       \n",
       "\n",
       "                                                                                                                                                              Embargo Dates  \\\n",
       "0  [2017-10-06, 2017-10-07, 2017-10-08, 2017-10-09, 2017-10-10, 2017-10-11, 2017-10-12, 2017-10-13, 2017-10-14, 2017-10-15, 2017-10-16, 2017-10-17, 2017-10-18, 2017-10-19]   \n",
       "1  [2017-10-15, 2017-10-16, 2017-10-17, 2017-10-18, 2017-10-19, 2017-10-20, 2017-10-21, 2017-10-22, 2017-10-23, 2017-10-24, 2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28]   \n",
       "2  [2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28, 2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02, 2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07]   \n",
       "3  [2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07, 2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11, 2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16]   \n",
       "4  [2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16, 2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25]   \n",
       "5  [2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25, 2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04]   \n",
       "6  [2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04, 2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09, 2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13]   \n",
       "7  [2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13, 2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18, 2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23]   \n",
       "8  [2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23, 2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27, 2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]               \n",
       "9  [2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]                                                                                                                           \n",
       "\n",
       "                                                                                                                                                       Lagged Embargo Dates  \n",
       "0  [2017-10-20, 2017-10-21, 2017-10-22, 2017-10-23, 2017-10-24, 2017-10-25, 2017-10-26, 2017-10-27, 2017-10-28, 2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02]  \n",
       "1  [2017-10-29, 2017-10-30, 2017-10-31, 2017-11-01, 2017-11-02, 2017-11-03, 2017-11-04, 2017-11-05, 2017-11-06, 2017-11-07, 2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11]  \n",
       "2  [2017-11-08, 2017-11-09, 2017-11-10, 2017-11-11, 2017-11-12, 2017-11-13, 2017-11-14, 2017-11-15, 2017-11-16, 2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21]  \n",
       "3  [2017-11-17, 2017-11-18, 2017-11-19, 2017-11-20, 2017-11-21, 2017-11-22, 2017-11-23, 2017-11-24, 2017-11-25, 2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30]  \n",
       "4  [2017-11-26, 2017-11-27, 2017-11-28, 2017-11-29, 2017-11-30, 2017-12-01, 2017-12-02, 2017-12-03, 2017-12-04, 2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09]  \n",
       "5  [2017-12-05, 2017-12-06, 2017-12-07, 2017-12-08, 2017-12-09, 2017-12-10, 2017-12-11, 2017-12-12, 2017-12-13, 2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18]  \n",
       "6  [2017-12-14, 2017-12-15, 2017-12-16, 2017-12-17, 2017-12-18, 2017-12-19, 2017-12-20, 2017-12-21, 2017-12-22, 2017-12-23, 2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27]  \n",
       "7  [2017-12-24, 2017-12-25, 2017-12-26, 2017-12-27, 2017-12-28, 2017-12-29, 2017-12-30, 2017-12-31]                                                                          \n",
       "8  []                                                                                                                                                                        \n",
       "9  []                                                                                                                                                                        "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection_dates = pd.DataFrame(model_selection_dates, columns=['Validation Date', 'Embargo Dates', 'Lagged Embargo Dates']).sort_values('Validation Date')\n",
    "model_selection_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_validation_subsets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_cols = ['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'date', 'day_of_week', \n",
    "               'mainCategory', 'category', 'subCategory', 'releaseDate', \n",
    "               'rrp', 'price', 'month',\n",
    "               'last_15_day_sales', 'last_16_day_sales', 'last_17_day_sales', 'last_18_day_sales', 'last_19_day_sales', 'last_20_day_sales', 'last_21_day_sales', \n",
    "               'last_22_day_sales', 'last_23_day_sales', 'last_24_day_sales', 'last_25_day_sales', 'last_26_day_sales', 'last_27_day_sales', 'last_28_day_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_Y_cols = ['key', 'date', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(validation_dates)):\n",
    "    full_embargo_set = set(validation_dates[i] + embargo_dates[i] + lagged_embargo_dates[i])\n",
    "    validation_date = validation_dates[i]\n",
    "    \n",
    "    X_train_subsets.append(X_full_train.loc[X_full_train['date'].apply(lambda x: x not in full_embargo_set)].drop(drop_X_cols, axis=1).as_matrix())\n",
    "    Y_train_subsets.append(Y_full_train.loc[Y_full_train['date'].apply(lambda x: x not in full_embargo_set)].drop(drop_Y_cols, axis=1).as_matrix())\n",
    "\n",
    "    X_validation_subsets.append(X_full_train.loc[X_full_train['date'].apply(lambda x: x in validation_date)].drop(drop_X_cols, axis=1).as_matrix())\n",
    "    Y_validation_subsets.append(Y_full_train.loc[Y_full_train['date'].apply(lambda x: x in validation_date)].drop(drop_Y_cols, axis=1).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dates = pd.DataFrame(X_full['key']).join(X_full['date']) # Store for future lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_full_train.drop(drop_X_cols, axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_full_train.drop(drop_Y_cols, axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Ridge, Lasso, LinearRegression, XGBRegressor, GradientBoostingRegressor, MLPRegressor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_called = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_hyperparameters_options = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*Ridge_hyperparameters_options.items())\n",
    "Ridge_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_hyperparameters_options = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*Lasso_hyperparameters_options.items())\n",
    "Lasso_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression_hyperparameters_options = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'n_jobs': [cpus],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*LinearRegression_hyperparameters_options.items())\n",
    "LinearRegression_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_gbtree_options = {\n",
    "    'booster': ['gbtree'],\n",
    "    'n_jobs': [cpus],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [40, 65, 100],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    #'max_depth ': [5, 10, 15, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_gbtree_options.items())\n",
    "XGBoost_hyperparameters_gbtree = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_gblinear_options = {\n",
    "    'booster': ['gblinear'],\n",
    "    'n_jobs': [cpus],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [40, 65, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_gblinear_options.items())\n",
    "XGBoost_hyperparameters_gblinear = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters_dart_options = {\n",
    "    'booster': ['dart'],\n",
    "    'n_jobs': [cpus],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [40, 65, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*XGBoost_hyperparameters_dart_options.items())\n",
    "XGBoost_hyperparameters_dart = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_hyperparameters = XGBoost_hyperparameters_gbtree + XGBoost_hyperparameters_gblinear + XGBoost_hyperparameters_dart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingRegressor Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingRegressor_hyperparameters_options = {\n",
    "    #'learning_rate': [0.01, 0.1, 0.3],\n",
    "    #'n_estimators': [40, 65, 100],\n",
    "    'max_depth': [3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*GradientBoostingRegressor_hyperparameters_options.items())\n",
    "GradientBoostingRegressor_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPRegressor Hyperparameters to Try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPRegressor_hyperparameters_options = {\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(125, )],\n",
    "    #'batch_size': ['auto', 10, 20, 40, 60, 80, 100],\n",
    "    #'max_iter': [10, 50, 100, 200],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*MLPRegressor_hyperparameters_options.items())\n",
    "MLPRegressor_hyperparameters = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Settings to Try for all Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameters = [Ridge_hyperparameters,\n",
    "                         Lasso_hyperparameters,\n",
    "                         LinearRegression_hyperparameters,\n",
    "                         XGBoost_hyperparameters,\n",
    "                         GradientBoostingRegressor_hyperparameters,\n",
    "                         MLPRegressor_hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_avg_rmse_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_models_txt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.3, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.4, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.6, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.7, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.8, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.9, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.2, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.3, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.4, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.5, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.6, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.7, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.8, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=0.9, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Ridge(alpha=1.0, copy_X=True, fit_intercept=False, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)]\n",
      "[1.1743452032919757]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1627947068891242]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.2, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1688419873962164]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.3, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1773522910869416]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.4, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1882655475115467]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1977370365179911]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.6, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2121945832561396]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.7, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2316682723913153]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.8, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2482968376917107]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.9, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2648132234105511]\n",
      "\n",
      "\n",
      "[Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2799048080836182]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1621659475363946]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.2, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1688008975449686]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.3, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1772611253183105]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.4, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1871372100394804]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.5, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.1979101064201605]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.6, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2120989178389165]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.7, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2309856024355519]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.8, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.247300926246987]\n",
      "\n",
      "\n",
      "[Lasso(alpha=0.9, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2639453815259238]\n",
      "\n",
      "\n",
      "[Lasso(alpha=1.0, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)]\n",
      "[1.2801217510939373]\n",
      "\n",
      "\n",
      "[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False)]\n",
      "[1.1743499735623701]\n",
      "\n",
      "\n",
      "[LinearRegression(copy_X=True, fit_intercept=False, n_jobs=8, normalize=False)]\n",
      "[1.174363223536659]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.2082441075822747]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.2056927578964898]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.1825751921251682]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1828052828724285]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.165106605747005]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1626858998000913]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.140646963691351]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1470972421860384]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.1517125255656475]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.150956259255166]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.1610510485328653]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.153779553292431]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.177599529664016]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1836633396585623]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.1771498613517213]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1819274598179816]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)]\n",
      "[1.188613824013793]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1.0)]\n",
      "[1.1781272777558516]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1425652422245656]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1540248776064301]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1606313326066366]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1720601729224627]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.17381248333906]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1741641048708211]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.174340051192805]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1743504749718083]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1743504749718083]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.2056927578964898]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1828052828724285]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1626858998000913]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1470972421860384]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.150956259255166]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.153779553292431]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1836633396585623]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=65,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1819274598179816]\n",
      "\n",
      "\n",
      "[XGBRegressor(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)]\n",
      "[1.1781272777558516]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianWeller\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)]\n",
      "[1.1587079644500515]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianWeller\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(125,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)]\n",
      "[1.1450585094891153]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianWeller\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(125,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)]\n",
      "[1.2114327875985609]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianWeller\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(125,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)]\n",
      "[1.2861784268806122]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JulianWeller\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(125,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)]\n",
      "[1.1995403326644252]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_id, model_val in enumerate(models):\n",
    "    model_hyperparameters_called = []\n",
    "    models_hyperparameters_avg_rmse_scores = []\n",
    "    \n",
    "    for model_hyperparameter_id, model_hyperparameter_val in enumerate(model_hyperparameters[model_id]):\n",
    "        model = model_val(**model_hyperparameter_val)\n",
    "        model_hyperparameters_called.append(model)\n",
    "        models_hyperparameters_rmse_score_subset = []\n",
    "        \n",
    "        for X_train_subset_id, X_train_subset_val in enumerate(X_train_subsets):\n",
    "            model.fit(X_train_subset_val, Y_train_subsets[X_train_subset_id])\n",
    "            models_hyperparameters_rmse_score_subset.append(sqrt(mean_squared_error(Y_validation_subsets[X_train_subset_id], np.round(model.predict(X_validation_subsets[X_train_subset_id])))))\n",
    "            \n",
    "        models_hyperparameters_avg_rmse_scores.append(np.average(models_hyperparameters_rmse_score_subset))\n",
    "\n",
    "        validated_models_txt.append(str(model_hyperparameters_called[-1:]) + \" | \" + str(models_hyperparameters_avg_rmse_scores[-1:]) + \"\\n\")\n",
    "        \n",
    "        print(model_hyperparameters_called[-1:])\n",
    "        print(models_hyperparameters_avg_rmse_scores[-1:])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    models_called.append(model_hyperparameters_called)\n",
    "    models_avg_rmse_scores.append(models_hyperparameters_avg_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_write_selected_models == 'w':\n",
    "    with open(working_directory + 'validated_models.txt', \"w\") as text_file:\n",
    "        text_file.write(str(validated_models_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter settings for the respective models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\n",
      "       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=0.7)\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(125,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "for models_avg_rmse_scores_id, models_avg_rmse_scores_val in enumerate(models_avg_rmse_scores):\n",
    "    selected_model = models_called[models_avg_rmse_scores_id][models_avg_rmse_scores_val.index(min(models_avg_rmse_scores_val))]\n",
    "    print(selected_model)\n",
    "    selected_models.append(selected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump/Read the best models to/from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_write_selected_models == 'w':\n",
    "    with open(working_directory + \"selected_models\", \"wb\") as fp:\n",
    "        pickle.dump(selected_models, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if read_write_selected_models == 'r':\n",
    "    with open(working_directory + \"selected_models\", \"rb\") as fp:\n",
    "        selected_models = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks Lu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup when using clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Import cluster identifier\n",
    "\n",
    "# sales = pd.read_csv(working_directory + 'data_v0.1_sales.csv')\n",
    "# big_key = sales['key'][sales['cluster'] == \"big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Import datasets\n",
    "\n",
    "# X_full = pickle.load(open(working_directory + 'X_flat.pkl', 'rb'))\n",
    "# Y_full = pickle.load(open(working_directory + 'Y_flat.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_cols = X_full.columns[12+14:12+14+14]\n",
    "#X_full = X_full.drop(drop_cols, axis=1)\n",
    "#print(X_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Find rows which belong to cluster 'big' in X_full\n",
    "\n",
    "# X_full['key'] = X_full['key'].astype(str)\n",
    "# X_big = X_full[X_full['key'].isin(big_key.astype(str))]\n",
    "# X_big = X_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Find rows which belong to cluster 'big' in Y_full\n",
    "\n",
    "# Y_full['key'] = Y_full['key'].astype(str)\n",
    "# Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))]\n",
    "# Y_big = Y_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    \n",
    "    # Split the X_big and Y_big into traing and test\n",
    "\n",
    "    X_big['month'] = pd.DatetimeIndex(X_big['date']).month\n",
    "    Y_big['month'] = pd.DatetimeIndex(Y_big['date']).month\n",
    "    X_big_train = X_big.loc[X_big['month'] != 1]\n",
    "    Y_train = Y_big.loc[Y_big['month'] != 1]['sales']\n",
    "    X_big_test = X_big.loc[X_big['month'] == 1]\n",
    "    Y_test = Y_big.loc[Y_big['month'] == 1]['sales']\n",
    "    \n",
    "    # Prepare the data for fitting the input of the model\n",
    "\n",
    "    drop_x_cols = ['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'date', 'day_of_week', \n",
    "                   'mainCategory', 'category', 'subCategory', 'releaseDate', \n",
    "                   'rrp', 'price', 'month']\n",
    "    X_train = X_big_train.drop(drop_x_cols, axis=1)\n",
    "    X_test = X_big_test.drop(drop_x_cols, axis=1)\n",
    "    #print(X_train.columns)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_train = np.delete(X_train, np.s_[14:28], axis=1)\n",
    "    Y_train = Y_train.as_matrix()\n",
    "    X_test = X_test.as_matrix()\n",
    "    X_test = np.delete(X_test, np.s_[14:28], axis=1)\n",
    "    Y_test = Y_test.as_matrix()\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, X_big_train, X_big_test, X_big, drop_x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, X_big_train, X_big_test, X_big, drop_x_cols = prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    test_data_sets.append(pd.read_csv(test_data_directory + 'test_' + str(i) + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(selected_models, test_data_sets, print_details):\n",
    "    \n",
    "    #selected_models_evaluation_scores_average = []\n",
    "    \n",
    "    #selected_models_evaluation_scores_std = []\n",
    "    \n",
    "    return_list = []\n",
    "    \n",
    "    len_selected_models = len(selected_models)\n",
    "\n",
    "    for selected_model_id, selected_model_val in enumerate(selected_models):\n",
    "        selected_model_val.fit(X_train, Y_train)\n",
    "        \n",
    "        test_data_results = []\n",
    "        \n",
    "        for test_data_set in test_data_sets:\n",
    "\n",
    "            # Test the model\n",
    "            # Only the sale unit of the first day for each item is right int the 'X_test'\n",
    "            # Select the row 'on Jan 1st'\n",
    "\n",
    "            X_Jan1 = X_test[0:1,:]\n",
    "            for i in range(int(len(X_test)/31-1)):\n",
    "                X_Jan1 = np.vstack([X_Jan1, X_test[(31+i*31):(32+i*31),:]])\n",
    "            if print_details:\n",
    "                print(X_Jan1)\n",
    "\n",
    "                print(X_Jan1.shape)\n",
    "\n",
    "            # Predict the sales unit 'on Jan 1st' for each items\n",
    "            # Change the format of the prediction results on Jan_1st\n",
    "\n",
    "            Y_Jan1 = selected_model_val.predict(X_Jan1)\n",
    "            prediction_1 = np.asarray([round(value) for value in Y_Jan1])\n",
    "            prediction_1 = np.reshape(prediction_1, (len(prediction_1),1))\n",
    "            if print_details:\n",
    "                print(prediction_1)\n",
    "\n",
    "            # Delete the 'last_28_day_sales'\n",
    "            # Add the prediction results as the 'last_1_day_sales'\n",
    "\n",
    "            X_Jan = X_Jan1\n",
    "            X_Jan = np.delete(X_Jan, np.s_[13:14], axis=1)\n",
    "            X_Jan = np.append(prediction_1, X_Jan, axis=1)\n",
    "            if print_details:\n",
    "                print(X_Jan)\n",
    "\n",
    "            # Add the process above into a loop\n",
    "            # Predict the sales units eery day in January for each item\n",
    "\n",
    "            predictions = prediction_1\n",
    "            for i in range(30):\n",
    "                Y_Jan = selected_model_val.predict(X_Jan)\n",
    "                prediction = np.asarray([round(value) for value in Y_Jan])\n",
    "                prediction = np.reshape(prediction, (len(prediction),1))\n",
    "                predictions = np.append(predictions, prediction, axis=1)\n",
    "                X_Jan = np.delete(X_Jan, np.s_[13:14], axis=1)\n",
    "                X_Jan = np.append(prediction, X_Jan, axis=1)\n",
    "            if print_details:\n",
    "                print(predictions)\n",
    "\n",
    "            # Reshape predictions: row ->'big' items, columns -> date\n",
    "\n",
    "            column_name = X_big_test['date'].unique().astype(str)\n",
    "            row_name = X_big_test['key'].unique().astype(str)\n",
    "\n",
    "            # Aggregate sales for each day each item.\n",
    "\n",
    "            pred_agg = predictions\n",
    "            agg_sum = predictions[:,0]\n",
    "            for i in range(len(column_name)-1):\n",
    "                agg_sum = pred_agg[:,i] + predictions[:, i+1]\n",
    "                pred_agg[:, i+1] = agg_sum\n",
    "            if print_details:\n",
    "                print(pred_agg)\n",
    "\n",
    "            # Add 'key' for test data by merging 'pid' and 'size'\n",
    "            # Select useful attributes\n",
    "\n",
    "            test_data_set[\"key\"] = test_data_set[\"pid\"].map(int).map(str) + test_data_set[\"size\"]\n",
    "            test_0_big = test_data_set.loc[test_data_set['key'].isin(row_name)]\n",
    "            subtest_0_big = test_0_big[['key','stock','sold_out_date']]\n",
    "            test = np.asarray(subtest_0_big)\n",
    "            if print_details:\n",
    "                print(test)\n",
    "\n",
    "            # Define arrays for storing the predicted day and date\n",
    "\n",
    "            pred_day = np.zeros((len(test),1), dtype=int)\n",
    "            pred_date = np.asarray(test_0_big[['sold_out_date']])\n",
    "\n",
    "            for i in range(len(test)):\n",
    "\n",
    "                # 'key' is the key for each test item\n",
    "                key = test[i,0]\n",
    "                # Find the index of the item in predictions sharing the same key\n",
    "                index = 0\n",
    "\n",
    "                # Match the items in 'test' with the items in 'pred_agg'\n",
    "                # Retern the index of the item in the 'pred_agg'\n",
    "                for j in range(len(row_name)):\n",
    "                    if row_name[j] == key:\n",
    "                        index = j\n",
    "                        break\n",
    "\n",
    "                # Match\n",
    "                if test[i,1] < pred_agg[index,0]:\n",
    "                    pred_day[i,0] = 1\n",
    "                    pred_date[i,0] = column_name[0]\n",
    "                    continue\n",
    "                if test[i,1] > pred_agg[index,30]:\n",
    "                    pred_day[i,0] = 15\n",
    "                    pred_date[i,0] = column_name[14]\n",
    "                    continue\n",
    "                for k in range(len(pred_agg[0])):\n",
    "                    if pred_agg[index,k] - test[i,1] >=0:\n",
    "                        pred_day[i,0] = k+1\n",
    "                        pred_date[i,0] = column_name[k]\n",
    "                        break\n",
    "\n",
    "            # Find the day of the real sold out date\n",
    "\n",
    "            day = [i[-2:] for i in test[:,2].tolist()]\n",
    "            real_day = np.reshape(np.asarray(list(map(int,day))), (len(test),1))\n",
    "            if print_details:\n",
    "                print(real_day)\n",
    "\n",
    "            # Show the predicted sold out date\n",
    "            if print_details:\n",
    "                print(pred_day)\n",
    "\n",
    "            # Error calculation\n",
    "\n",
    "            error = sqrt(np.sum(np.abs(np.subtract(pred_day, real_day))))\n",
    "            if print_details:\n",
    "                print(error)\n",
    "            test_data_results.append(error)\n",
    "\n",
    "            # Visualize the result in dataframe\n",
    "\n",
    "            result = np.append(test, pred_date, axis=1)\n",
    "            result_column =['key','stock','real_sold_out_day','predicted_sold_out_day']\n",
    "            prediction_result = pd.DataFrame(result, columns=result_column)\n",
    "            if print_details:\n",
    "                prediction_result\n",
    "        \n",
    "        #selected_models_evaluation_scores_average.append(np.average(test_data_results))\n",
    "        #selected_models_evaluation_scores_std.append(np.std(test_data_results))\n",
    "        \n",
    "        return_list.append([selected_model_val, np.average(test_data_results), np.std(test_data_results)])\n",
    "        \n",
    "        print(\"Finished with model \" + str(selected_model_id + 1) + \" out of \" + str(len_selected_models))\n",
    "        \n",
    "    return return_list"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with model 1 out of 6\n",
      "Finished with model 2 out of 6\n",
      "Finished with model 3 out of 6\n",
      "Finished with model 4 out of 6\n",
      "Finished with model 5 out of 6\n",
      "Finished with model 6 out of 6\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluation(selected_models, test_data_sets, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_txt = [([str(j) for j in i]) for i in evaluation_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\\n       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\\n       hidden_layer_sizes=(125,), learning_rate='constant',\\n       learning_rate_init=0.001, max_iter=200, momentum=0.9,\\n       nesterovs_momentum=True, power_t=0.5, random_state=None,\\n       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\\n       verbose=False, warm_start=False)</td>\n",
       "      <td>257.9774613257222</td>\n",
       "      <td>0.17160400793167344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\\n             min_impurity_split=None, min_samples_leaf=1,\\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\\n             n_estimators=100, presort='auto', random_state=None,\\n             subsample=1.0, verbose=0, warm_start=False)</td>\n",
       "      <td>258.1099728101331</td>\n",
       "      <td>0.20478269542106065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=1000,\\n   normalize=False, positive=False, precompute=False, random_state=None,\\n   selection='cyclic', tol=0.0001, warm_start=False)</td>\n",
       "      <td>258.1727514776515</td>\n",
       "      <td>0.1743400665260544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\\n   normalize=False, random_state=None, solver='auto', tol=0.001)</td>\n",
       "      <td>258.2486379949883</td>\n",
       "      <td>0.2024196963371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False)</td>\n",
       "      <td>258.2486379949883</td>\n",
       "      <td>0.2024196963371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\\n       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\\n       silent=True, subsample=0.7)</td>\n",
       "      <td>258.404996803443</td>\n",
       "      <td>0.240056269650017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Model  \\\n",
       "5  MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\\n       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\\n       hidden_layer_sizes=(125,), learning_rate='constant',\\n       learning_rate_init=0.001, max_iter=200, momentum=0.9,\\n       nesterovs_momentum=True, power_t=0.5, random_state=None,\\n       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\\n       verbose=False, warm_start=False)                     \n",
       "4  GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\\n             min_impurity_split=None, min_samples_leaf=1,\\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\\n             n_estimators=100, presort='auto', random_state=None,\\n             subsample=1.0, verbose=0, warm_start=False)   \n",
       "1  Lasso(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=1000,\\n   normalize=False, positive=False, precompute=False, random_state=None,\\n   selection='cyclic', tol=0.0001, warm_start=False)                                                                                                                                                                                                                                                                              \n",
       "0  Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\\n   normalize=False, random_state=None, solver='auto', tol=0.001)                                                                                                                                                                                                                                                                                                                                             \n",
       "2  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=8, normalize=False)                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=40,\\n       n_jobs=8, nthread=None, objective='reg:linear', random_state=0,\\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\\n       silent=True, subsample=0.7)                                                                            \n",
       "\n",
       "                 Avg               Stddev  \n",
       "5  257.9774613257222  0.17160400793167344  \n",
       "4  258.1099728101331  0.20478269542106065  \n",
       "1  258.1727514776515  0.1743400665260544   \n",
       "0  258.2486379949883  0.2024196963371918   \n",
       "2  258.2486379949883  0.2024196963371918   \n",
       "3  258.404996803443   0.240056269650017    "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results_df = pd.DataFrame(evaluation_results_txt, columns=['Model', 'Avg', 'Stddev']).sort_values('Avg')\n",
    "evaluation_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

