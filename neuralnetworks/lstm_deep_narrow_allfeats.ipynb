{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook has the following sections:\n",
    "1. Import full dataset and keep only big cluster\n",
    "2. Model helpers and parameters\n",
    "3. Model definition\n",
    "4. Model training\n",
    "5. Prediction\n",
    "6. Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamlcc\\datamining2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import full dataset and keep only big cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have the X_big and Y_big dataset, skip this part and go to 'Load big cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2907\n2      100033 (35-38 )\n38              10063L\n39              10063M\n40             10063XL\n48    100652 ( 37-39 )\nName: key, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import cluster identifier\n",
    "sales = pd.read_csv('./data/clean/data_v0.1_sales.csv')\n",
    "big_key = sales['key'][sales['cluster'] == \"big\"]\n",
    "print(len(big_key.unique())) # Should only have 2907 keys remaining\n",
    "print(big_key.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1577352, 108)\n(1577352, 3)\nIndex(['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'mainCategory',\n       'category', 'subCategory', 'releaseDate',\n       ...\n       'cat_7', 'cat_10', 'cat_16', 'cat_18', 'cat_24', 'cat_30', 'cat_33',\n       'cat_36', 'cat_37', 'marketing_activity'],\n      dtype='object', length=108)\nIndex(['key', 'date', 'sales'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import datasets\n",
    "X_full = pickle.load(open('./data/clean/X_flat.pkl', 'rb'))\n",
    "Y_full = pickle.load(open('./data/clean/Y_flat.pkl', 'rb'))\n",
    "\n",
    "print(X_full.shape)\n",
    "print(Y_full.shape)\n",
    "print(X_full.columns)\n",
    "print(Y_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357561, 108) (357561, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       key  pid_x         size_x color brand    rrp  \\\n357556  22872M ( 140-152 )  22872  M ( 140-152 )  blau  Nike  50.73   \n357557  22872M ( 140-152 )  22872  M ( 140-152 )  blau  Nike  50.73   \n357558  22872M ( 140-152 )  22872  M ( 140-152 )  blau  Nike  50.73   \n357559  22872M ( 140-152 )  22872  M ( 140-152 )  blau  Nike  50.73   \n357560  22872M ( 140-152 )  22872  M ( 140-152 )  blau  Nike  50.73   \n\n        mainCategory  category  subCategory releaseDate         ...          \\\n357556             1         7         22.0  2017-10-01         ...           \n357557             1         7         22.0  2017-10-01         ...           \n357558             1         7         22.0  2017-10-01         ...           \n357559             1         7         22.0  2017-10-01         ...           \n357560             1         7         22.0  2017-10-01         ...           \n\n       cat_7  cat_10  cat_16  cat_18  cat_24  cat_30  cat_33  cat_36  cat_37  \\\n357556     1       0       0       0       0       0       0       0       0   \n357557     1       0       0       0       0       0       0       0       0   \n357558     1       0       0       0       0       0       0       0       0   \n357559     1       0       0       0       0       0       0       0       0   \n357560     1       0       0       0       0       0       0       0       0   \n\n        marketing_activity  \n357556                   0  \n357557                   0  \n357558                   0  \n357559                   1  \n357560                   0  \n\n[5 rows x 108 columns] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       key        date  sales\n357556  22872M ( 140-152 )  2018-01-27    0.0\n357557  22872M ( 140-152 )  2018-01-28    0.0\n357558  22872M ( 140-152 )  2018-01-29    0.0\n357559  22872M ( 140-152 )  2018-01-30    0.0\n357560  22872M ( 140-152 )  2018-01-31    0.0\n"
     ]
    }
   ],
   "source": [
    "# Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "X_big = X_full[X_full['key'].isin(big_key.astype(str))].reset_index(drop=True)\n",
    "Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))].reset_index(drop=True)\n",
    "print(X_big.shape, Y_big.shape)\n",
    "print(X_big.tail(), Y_big.tail()) # Check that order is still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to pickle for faster loading of data in future\n",
    "pickle.dump(X_big, open('./data/clean/X_big.pkl', 'wb'))\n",
    "pickle.dump(Y_big, open('./data/clean/Y_big.pkl', 'wb'))\n",
    "# Read the file later by: X_big = pickle.load(open('path/to/X_big.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load big cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357561, 108)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(X_big.shape)\n",
    "except:\n",
    "    X_big = pickle.load(open('./data/clean/X_big.pkl', 'rb'))\n",
    "    Y_big = pickle.load(open('./data/clean/Y_big.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357561, 95) (357561, 1)\n"
     ]
    }
   ],
   "source": [
    "# Drop non-numeric columns; networks only take numeric input\n",
    "keys_dates = pd.DataFrame(X_big['key']).join(X_big['date']) # Store for future lookups\n",
    "drop_x_cols = ['key', 'pid_x', 'size_x', 'color', 'brand', 'rrp', 'date', 'day_of_week', \n",
    "               'mainCategory', 'category', 'subCategory', 'releaseDate', \n",
    "               'rrp', 'price']\n",
    "drop_y_cols = ['key', 'date']\n",
    "X = X_big.drop(drop_x_cols, axis=1)\n",
    "Y = Y_big.drop(drop_y_cols, axis=1)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'as_matrix'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cea601692b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert to numpy to reshape for input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Each row has shape (num_vars,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Each row has shape (1,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'as_matrix'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Convert to numpy to reshape for input\n",
    "X = X.as_matrix() # Each row has shape (num_vars,)\n",
    "Y = Y.as_matrix() # Each row has shape (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model helpers and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a data generator to create mini-batches of X, Y for us\n",
    "# For training, X = [0:30], [1:31], ..., [62:92] (total 63 slices per product)\n",
    "# For training, Y = [29], [30], ..., [91]\n",
    "# For testing, X = [63:93], [64:94], ..., [93:123] (total 31 slices per product)\n",
    "# For testing, Y = [92], [93], ..., [122]\n",
    "# Check whether slices are correct!\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, num_samples, X, Y, batch_size, window_size, num_vars, start_day, end_day):\n",
    "        self.num_samples = num_samples\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_vars = num_vars\n",
    "        self.start_day = start_day # 0 = day 1 (2017-10-01)\n",
    "        self.end_day = end_day\n",
    "        self.current_product = 0 # To keep track of which product we are at (of 12,824)\n",
    "        self.current_day = 0 # To track which day of a product we are in (of 123)\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.window_size, self.num_vars))\n",
    "        y = np.zeros((self.batch_size, 1)) # Should this be 3d?\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if (self.current_day+self.start_day+self.window_size) > self.end_day:\n",
    "                    # Go to next product, first day\n",
    "                    self.current_product += 1\n",
    "                    self.current_day = 0\n",
    "                if self.current_product == self.num_samples:\n",
    "                    # Go back to first product for next epoch\n",
    "                    self.current_product = 0\n",
    "                x[i,:,:] = self.X[self.current_product*123+self.current_day+self.start_day:\n",
    "                                  self.current_product*123+self.current_day+self.start_day+\n",
    "                                  self.window_size] # Max = [62:92]\n",
    "                y[i,:] = self.Y[self.current_product*123+self.current_day+self.start_day+self.window_size-1] # Max = [91]\n",
    "                self.current_day += 1 \n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 2907 # Total number of samples (you can use all or just a subset)\n",
    "WINDOW_SIZE = 30\n",
    "num_vars = X.shape[1]\n",
    "train_batch_size = 92 - WINDOW_SIZE + 1 # To make 1 product = 1 batch\n",
    "# test_batch_size = 31 # To make 1 product = 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_day = 0\n",
    "train_end_day = 92 # 2017-10-01 to 2017-12-31\n",
    "# test_start_day = 92 - WINDOW_SIZE + 1 # So the first window frame ends with 2018-01-01 prediction\n",
    "# test_end_day = 123 # 2018-01-01 to 2018-01-31\n",
    "\n",
    "train_data_generator = BatchGenerator(NUM_SAMPLES, X, Y, train_batch_size, WINDOW_SIZE, \n",
    "                                      num_vars, train_start_day, train_end_day)\n",
    "# test_data_generator = BatchGenerator(NUM_SAMPLES, X, Y, test_batch_size, WINDOW_SIZE, \n",
    "#                                      num_vars, test_start_day, test_end_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define customised metric calculations here if we need\n",
    "# Metric function is similar to a loss function\n",
    "# except that the results from evaluating a metric are not used when training the model\n",
    "import keras.backend as backend\n",
    "def mean_abs_diff(y_true, y_pred):\n",
    "    return backend.mean(backend.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, if you already have a model trained previously, \n",
    "## you can skip to '5. Prediction' to load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_7 (LSTM)                (None, 30, 32)            16384     \n_________________________________________________________________\nlstm_8 (LSTM)                (None, 30, 32)            8320      \n_________________________________________________________________\nlstm_9 (LSTM)                (None, 32)                8320      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 33,057\nTrainable params: 33,057\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "# Using 5-layered MLP (idea from presentation by Matthias' team)\n",
    "# Windowing model\n",
    "# Model predicts only sales unit of last day of a window frame\n",
    "# Model definition\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed, LSTM, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 50 # This is too low; can increase if we push training to the cloud\n",
    "num_hidden = 32\n",
    "# Monitor 'loss' instead of 'val_loss', because there is no way to validate actually\n",
    "callbacks_list = [EarlyStopping(monitor='loss', patience=10), \n",
    "                  ModelCheckpoint(filepath='./data/models/lstm_deep_narrow_allfeats_best.h5', monitor='loss',save_best_only=True)]\n",
    "# ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='min', cooldown=0, min_lr=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_hidden, input_shape=(WINDOW_SIZE, num_vars), activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(num_hidden, input_shape=(WINDOW_SIZE, num_hidden), activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(num_hidden, input_shape=(WINDOW_SIZE, num_hidden), activation='tanh', return_sequences=False))\n",
    "model.add(Dense(1, activation='relu')) # Need kernel_initializer?\n",
    "model.compile(loss='mean_squared_error', optimizer='adadelta') # To penalise big deviations more\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bb9e7b1a5c15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train LSTM; running this will take a while!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = model.fit_generator(generator=train_data_generator.generate(),\n\u001b[0m\u001b[0;32m      3\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_SAMPLES\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_end_day\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               \u001b[1;31m# validation_data=test_data_generator.generate(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                               \u001b[1;31m# validation_steps=NUM_SAMPLES*(test_end_day-test_start_day-WINDOW_SIZE+1)/test_batch_size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train LSTM; running this will take a while!\n",
    "history = model.fit_generator(generator=train_data_generator.generate(),\n",
    "                              steps_per_epoch=NUM_SAMPLES*(train_end_day-WINDOW_SIZE+1)/train_batch_size,\n",
    "                              # validation_data=test_data_generator.generate(),\n",
    "                              # validation_steps=NUM_SAMPLES*(test_end_day-test_start_day-WINDOW_SIZE+1)/test_batch_size, \n",
    "                              callbacks=callbacks_list,\n",
    "                              epochs = num_epochs, verbose=1,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and history for future reuse\n",
    "model.save('mlp_clusterbig.h5')\n",
    "with open('mlp_history_clusterbig', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) Predict sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('mlp_clusterbig_best.h5') # LOAD THE MODEL YOU WANT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sales (day1 to last day):\n[0.0, 3.0, 0.0, 4.0, 2.0, 10.0, 8.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0]\nActual sales (day1 to last day):\n[ 4.  7.  9.  5.  5.  2.  2.  4.  3.  9.  7.  5.  5.  5.  8.  2.  6.  6.\n  4.  2.  5. 55. 27. 44.  3.  0.  1.  2.  4. 20. 50.]\n"
     ]
    }
   ],
   "source": [
    "# For future predictions, we have to replace the actual past sales with the predicted sales \n",
    "sample_product = '19288L' # Change as required\n",
    "index = int((keys_dates.index[keys_dates['key'] == sample_product][0])/123) \n",
    "predictions = []\n",
    "product_data = np.zeros(shape=(1, 123, num_vars)) # A copy of the data for replacement\n",
    "product_data[0, :, :] = X[index*123:index*123+123]\n",
    "\n",
    "product_X = np.zeros(shape=(1, WINDOW_SIZE, num_vars)) # This will be pushed into model\n",
    "for i in range(31):\n",
    "    product_X[0, :, :] = product_data[0, 63+i:63+WINDOW_SIZE+i]\n",
    "    predictions.insert(0, round(model.predict(product_X)[0][0]))\n",
    "    # Update product data with predictions (we are not supposed to know the actual values)\n",
    "    if i == 0:\n",
    "        product_data[0, 93+i, 0] = predictions[0] # [93] = 2018-01-02 input\n",
    "    elif i < 28:\n",
    "        product_data[0, 93+i, 0:i+1] = predictions[0:i+1]\n",
    "    elif i < 30: # No need to update for i = 30 (last prediction day)\n",
    "        product_data[0, 93+i, 0:28] = predictions[0:28] # We keep taking the last 28 predictions\n",
    "predictions = predictions[::-1] # We have to reverse the list to get back original order\n",
    "print(\"Predicted sales (day1 to last day):\")\n",
    "print(predictions)\n",
    "print(\"Actual sales (day1 to last day):\")\n",
    "print(Y[index*123+92:index*123+123].reshape(31,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the sample output above. If predictions look odd, go back to 'Model definition' and 'Model training'. If predictions look ok, proceed to 'Predict all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) Predict all; takes about 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def pred_soldout_day(pred_sales, stock):\n",
    "    \"\"\"\n",
    "    Helper function to predict soldout day\n",
    "    \"\"\"\n",
    "    result = 15 if stock <= 3 else len(pred_sales) # If stock is too few, middle is a good guess\n",
    "    for day in range(len(pred_sales)):\n",
    "        stock -= pred_sales[day]\n",
    "        if stock <= 0:\n",
    "            return day+1\n",
    "    return result\n",
    "\n",
    "def load_test_set(set_number):\n",
    "    \"\"\"\n",
    "    Helper function to load and prepare test set\n",
    "    \"\"\"\n",
    "    test = pd.read_csv('./data/test/test_{}.csv'.format(str(set_number)), index_col=0, \n",
    "                       dtype={'pid': np.int32})\n",
    "    test['key'] = test['pid'].astype(str) + test['size']\n",
    "    test = test[test['key'].isin(big_key.astype(str))].reset_index(drop=True) # FOR BIG CLUSTER\n",
    "    print('Number of products in test data belonging to cluster: ' + str(len(test['key'].unique())))\n",
    "    \n",
    "    def extract_soldout_day(row):\n",
    "        soldout_day = row['sold_out_date'][-2:]\n",
    "        return int(soldout_day[1]) if soldout_day[0] == 0 else int(soldout_day)\n",
    "    \n",
    "    test['soldout_day'] = test.apply(extract_soldout_day, axis=1)\n",
    "    test['pred_soldout_day'] = np.nan\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for all products and store them\n",
    "test = load_test_set(0)\n",
    "all_predictions = pd.DataFrame()\n",
    "\n",
    "# We only make predictions for the products in the test set\n",
    "for row in range(test.shape[0]):\n",
    "    key = str(test.iloc[row]['key'])\n",
    "    \n",
    "    # Make predictions for current product\n",
    "    index = int(keys_dates[keys_dates['key'] == key].index[0]/123) # Get starting row of product\n",
    "    predictions = []\n",
    "    product_data = np.zeros(shape=(1, 123, num_vars)) # A copy of the data for replacement\n",
    "    product_data[0, :, :] = X[index*123:index*123+123]\n",
    "    \n",
    "    product_X = np.zeros(shape=(1, WINDOW_SIZE, num_vars)) # This will be pushed into model\n",
    "    for i in range(31):\n",
    "        product_X[0, :, :] = product_data[0, 63+i:63+WINDOW_SIZE+i]\n",
    "        predictions.insert(0, round(model.predict(product_X)[0][0]))\n",
    "        # Update product data with predictions (we are not supposed to know the actual values)\n",
    "        if i == 0:\n",
    "            product_data[0, 93+i, 0] = predictions[0] # [93] = 2018-01-02 input\n",
    "        elif i < 28:\n",
    "            product_data[0, 93+i, 0:i+1] = predictions[0:i+1]\n",
    "        elif i < 30: # No need to update for i = 30 (last prediction day)\n",
    "            product_data[0, 93+i, 0:28] = predictions[0:28] # Take only last 28 predictions\n",
    "    predictions = predictions[::-1] # Reverse the list to get back original order\n",
    "    all_predictions[key] = predictions\n",
    "\n",
    "all_predictions.to_csv('./data/test/pred_mlp_clusterbig.csv', index=False)\n",
    "print(\"Predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate on test data; takes about 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13. 16. 16.  7. 12. 10. 23. 27. 29. 35. 31. 31. 27. 22. 21. 20. 23. 23.\n 29. 34. 36. 36. 34. 30. 28. 26. 22. 20. 21. 25. 28.]\n4\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "prod_pred = all_predictions['12985L'].as_matrix()\n",
    "print(prod_pred)\n",
    "print(pred_soldout_day(prod_pred, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0: 155.33834040570923\n2637\n"
     ]
    }
   ],
   "source": [
    "# Go through all test sets\n",
    "for set_num in range(5):\n",
    "    test = load_test_set(set_num)\n",
    "    for row in range(test.shape[0]):\n",
    "        key = test['key'][row]\n",
    "        stock = test['stock'][row]\n",
    "        key_pred = all_predictions[key].as_matrix()\n",
    "        test['pred_soldout_day'][row] = pred_soldout_day(key_pred, stock)\n",
    "    test.to_csv('./data/test/mlp_clusterbig_{}_result.csv'.format(str(set_num))) # Dump test set result\n",
    "    print('Test {}: {}'.format(str(set_num), \n",
    "                               str(np.sqrt(sum(abs(test['pred_soldout_day'] - test['soldout_day']))))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
