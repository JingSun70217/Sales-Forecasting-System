{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook has the following sections:\n",
    "1. Import full dataset and keep only big cluster\n",
    "2. Model helpers and parameters\n",
    "3. Model definition\n",
    "4. Model training\n",
    "5. Prediction\n",
    "6. Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/d071503/Desktop/Uni/projects/datamining2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import full dataset and keep only big cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have the X_big and Y_big dataset, skip this part and go to 'Load big cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2907\n2      100033 (35-38 )\n38              10063L\n39              10063M\n40             10063XL\n48    100652 ( 37-39 )\nName: key, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import cluster identifier\n",
    "sales = pd.read_csv('./data/clean/data_v0.1_sales.csv')\n",
    "big_key = sales['key'][sales['cluster'] == \"big\"]\n",
    "print(len(big_key.unique())) # Should only have 2907 keys remaining\n",
    "print(big_key.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-1cdb0ca24012>, line 3)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-1cdb0ca24012>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Y_full = pickle.load(open('./data/clean/Y_flat.pkl', 'rb')\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Import datasets\n",
    "X_full = pickle.load(open('./data/clean/X_flat.pkl', 'rb')\n",
    "Y_full = pickle.load(open('./data/clean/Y_flat.pkl', 'rb')\n",
    "\n",
    "print(X_full.shape)\n",
    "print(Y_full.shape)\n",
    "print(X_full.columns)\n",
    "print(Y_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows which belong to cluster 'big'; should be 2,907*123 = 357,561 rows\n",
    "X_big = X_full[X_full['key'].isin(big_key.astype(str))].reset_index(drop=True)\n",
    "Y_big = Y_full[Y_full['key'].isin(big_key.astype(str))].reset_index(drop=True)\n",
    "print(X_big.shape, Y_big.shape)\n",
    "print(X_big.tail(), Y_big.tail()) # Check that order is still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to pickle for faster loading of data in future\n",
    "pickle.dump(X_big, open('./data/clean/X_big.pkl', 'wb'))\n",
    "pickle.dump(Y_big, open('./data/clean/Y_big.pkl', 'wb'))\n",
    "# Read the file later by: X_big = pickle.load(open('path/to/X_big.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load big cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357561, 108)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(X_big.shape)\n",
    "except:\n",
    "    X_big = pickle.load(open('./data/clean/X_big.pkl', 'rb'))\n",
    "    Y_big = pickle.load(open('./data/clean/Y_big.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_big manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['last_1_day_sales', 'last_2_day_sales', 'last_3_day_sales',\n",
      "       'last_4_day_sales', 'last_5_day_sales', 'last_6_day_sales',\n",
      "       'last_7_day_sales', 'last_8_day_sales', 'last_9_day_sales',\n",
      "       'last_10_day_sales', 'last_11_day_sales', 'last_12_day_sales',\n",
      "       'last_13_day_sales', 'last_14_day_sales', 'last_15_day_sales',\n",
      "       'last_16_day_sales', 'last_17_day_sales', 'last_18_day_sales',\n",
      "       'last_19_day_sales', 'last_20_day_sales', 'last_21_day_sales',\n",
      "       'last_22_day_sales', 'last_23_day_sales', 'last_24_day_sales',\n",
      "       'last_25_day_sales', 'last_26_day_sales', 'last_27_day_sales',\n",
      "       'last_28_day_sales'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "x_cols = X_big.columns[12:12+28]\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357561, 28) (357561, 1)\n",
      "   last_1_day_sales  last_2_day_sales  last_3_day_sales  last_4_day_sales  \\\n",
      "0               0.0               0.0               0.0               0.0   \n",
      "1               0.0               0.0               0.0               0.0   \n",
      "2               0.0               0.0               0.0               0.0   \n",
      "3               0.0               0.0               0.0               0.0   \n",
      "4               0.0               0.0               0.0               0.0   \n",
      "\n",
      "   last_5_day_sales  last_6_day_sales  last_7_day_sales  last_8_day_sales  \\\n",
      "0               0.0               0.0               0.0               0.0   \n",
      "1               0.0               0.0               0.0               0.0   \n",
      "2               0.0               0.0               0.0               0.0   \n",
      "3               0.0               0.0               0.0               0.0   \n",
      "4               0.0               0.0               0.0               0.0   \n",
      "\n",
      "   last_9_day_sales  last_10_day_sales        ...          last_19_day_sales  \\\n",
      "0               0.0                0.0        ...                        0.0   \n",
      "1               0.0                0.0        ...                        0.0   \n",
      "2               0.0                0.0        ...                        0.0   \n",
      "3               0.0                0.0        ...                        0.0   \n",
      "4               0.0                0.0        ...                        0.0   \n",
      "\n",
      "   last_20_day_sales  last_21_day_sales  last_22_day_sales  last_23_day_sales  \\\n",
      "0                0.0                0.0                0.0                0.0   \n",
      "1                0.0                0.0                0.0                0.0   \n",
      "2                0.0                0.0                0.0                0.0   \n",
      "3                0.0                0.0                0.0                0.0   \n",
      "4                0.0                0.0                0.0                0.0   \n",
      "\n",
      "   last_24_day_sales  last_25_day_sales  last_26_day_sales  last_27_day_sales  \\\n",
      "0                0.0                0.0                0.0                0.0   \n",
      "1                0.0                0.0                0.0                0.0   \n",
      "2                0.0                0.0                0.0                0.0   \n",
      "3                0.0                0.0                0.0                0.0   \n",
      "4                0.0                0.0                0.0                0.0   \n",
      "\n",
      "   last_28_day_sales  \n",
      "0                0.0  \n",
      "1                0.0  \n",
      "2                0.0  \n",
      "3                0.0  \n",
      "4                0.0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop non-numeric columns; networks only take numeric input\n",
    "keys_dates = pd.DataFrame(X_big['key']).join(X_big['date']) # Store for future lookups\n",
    "\n",
    "drop_y_cols = ['key', 'date']\n",
    "X = X_big[x_cols]\n",
    "Y = Y_big.drop(drop_y_cols, axis=1)\n",
    "print(X.shape, Y.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'as_matrix'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5451c3b2e9d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert to numpy to reshape for input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Each row has shape (num_vars,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Each row has shape (1,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'as_matrix'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Convert to numpy to reshape for input\n",
    "X = X.as_matrix() # Each row has shape (num_vars,)\n",
    "Y = Y.as_matrix() # Each row has shape (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model helpers and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a data generator to create mini-batches of X, Y for us\n",
    "# For training, X = [0:30], [1:31], ..., [62:92] (total 63 slices per product)\n",
    "# For training, Y = [29], [30], ..., [91]\n",
    "# For testing, X = [63:93], [64:94], ..., [93:123] (total 31 slices per product)\n",
    "# For testing, Y = [92], [93], ..., [122]\n",
    "# Check whether slices are correct!\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, num_samples, X, Y, batch_size, window_size, num_vars, start_day, end_day):\n",
    "        self.num_samples = num_samples\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_vars = num_vars\n",
    "        self.start_day = start_day # 0 = day 1 (2017-10-01)\n",
    "        self.end_day = end_day\n",
    "        self.current_product = 0 # To keep track of which product we are at (of 12,824)\n",
    "        self.current_day = 0 # To track which day of a product we are in (of 123)\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.window_size, self.num_vars))\n",
    "        y = np.zeros((self.batch_size, 1)) # Should this be 3d?\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if (self.current_day+self.start_day+self.window_size) > self.end_day:\n",
    "                    # Go to next product, first day\n",
    "                    self.current_product += 1\n",
    "                    self.current_day = 0\n",
    "                if self.current_product == self.num_samples:\n",
    "                    # Go back to first product for next epoch\n",
    "                    self.current_product = 0\n",
    "                x[i,:,:] = self.X[self.current_product*123+self.current_day+self.start_day:\n",
    "                                  self.current_product*123+self.current_day+self.start_day+\n",
    "                                  self.window_size] # Max = [62:92]\n",
    "                y[i,:] = self.Y[self.current_product*123+self.current_day+self.start_day+self.window_size-1] # Max = [91]\n",
    "                self.current_day += 1 \n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 2907 # Total number of samples (you can use all or just a subset)\n",
    "WINDOW_SIZE = 30\n",
    "num_vars = X.shape[1]\n",
    "train_batch_size = 63 # To make 1 product = 1 batch\n",
    "# test_batch_size = 31 # To make 1 product = 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_day = 0\n",
    "train_end_day = 92 # 2017-10-01 to 2017-12-31\n",
    "# test_start_day = 92 - WINDOW_SIZE + 1 # So the first window frame ends with 2018-01-01 prediction\n",
    "# test_end_day = 123 # 2018-01-01 to 2018-01-31\n",
    "\n",
    "train_data_generator = BatchGenerator(NUM_SAMPLES, X, Y, train_batch_size, WINDOW_SIZE, \n",
    "                                      num_vars, train_start_day, train_end_day)\n",
    "# test_data_generator = BatchGenerator(NUM_SAMPLES, X, Y, test_batch_size, WINDOW_SIZE, \n",
    "#                                      num_vars, test_start_day, test_end_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define customised metric calculations here if we need\n",
    "# Metric function is similar to a loss function\n",
    "# except that the results from evaluating a metric are not used when training the model\n",
    "import keras.backend as backend\n",
    "def mean_abs_diff(y_true, y_pred):\n",
    "    return backend.mean(backend.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, if you already have a model trained previously, \n",
    "## you can skip to '5. Prediction' to load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 30, 64)            6144      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 30, 64)            4160      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 30, 64)            4160      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 30, 64)            4160      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 1921      \n",
      "=================================================================\n",
      "Total params: 20,545\n",
      "Trainable params: 20,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using 5-layered MLP (idea from presentation by Matthias' team)\n",
    "# Windowing model\n",
    "# Model predicts only sales unit of last day of a window frame\n",
    "# Model definition\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed, LSTM, Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 50 # This is too low; can increase if we push training to the cloud\n",
    "num_hidden = 64\n",
    "# Monitor 'loss' instead of 'val_loss', because there is no way to validate actually\n",
    "callbacks_list = [EarlyStopping(monitor='loss', patience=10), \n",
    "                  ModelCheckpoint(filepath='./data/models/mlp_clusterbig_best.h5', monitor='loss',save_best_only=True)]\n",
    "# ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='min', cooldown=0, min_lr=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_hidden, input_shape=(WINDOW_SIZE, num_vars), activation='relu'))\n",
    "model.add(Dense(num_hidden, activation='relu'))\n",
    "model.add(Dense(num_hidden, activation='relu'))\n",
    "model.add(Dense(num_hidden, activation='relu')) # Add dropout?\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='relu')) # Need kernel_initializer?\n",
    "model.compile(loss='mean_squared_error', optimizer='adadelta') # To penalise big deviations more\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 9.5048\n",
      "Epoch 2/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 7.9120\n",
      "Epoch 3/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 7.8509\n",
      "Epoch 4/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 6.5079\n",
      "Epoch 5/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 5.2245\n",
      "Epoch 6/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 6.4792\n",
      "Epoch 7/50\n",
      "2907/2907 [==============================] - 23s 8ms/step - loss: 6.3200\n",
      "Epoch 8/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 6.0555\n",
      "Epoch 9/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.5990\n",
      "Epoch 10/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.5610\n",
      "Epoch 11/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.8300\n",
      "Epoch 12/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.1995\n",
      "Epoch 13/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.5841\n",
      "Epoch 14/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.5844\n",
      "Epoch 15/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.0950\n",
      "Epoch 16/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.3067\n",
      "Epoch 17/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 5.5342\n",
      "Epoch 18/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.9988\n",
      "Epoch 19/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.9103\n",
      "Epoch 20/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 4.9692\n",
      "Epoch 21/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 5.1150\n",
      "Epoch 22/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.8597\n",
      "Epoch 23/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 4.8539\n",
      "Epoch 24/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.7515\n",
      "Epoch 25/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.1670\n",
      "Epoch 26/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.9647\n",
      "Epoch 27/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.4442\n",
      "Epoch 28/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.8604\n",
      "Epoch 29/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.8311\n",
      "Epoch 30/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.3245\n",
      "Epoch 31/50\n",
      "2907/2907 [==============================] - 25s 9ms/step - loss: 5.2315\n",
      "Epoch 32/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 5.1287\n",
      "Epoch 33/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.4728\n",
      "Epoch 34/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.5717\n",
      "Epoch 35/50\n",
      "2907/2907 [==============================] - 26s 9ms/step - loss: 4.7505\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM; running this will take a while!\n",
    "history = model.fit_generator(generator=train_data_generator.generate(),\n",
    "                              steps_per_epoch=NUM_SAMPLES*(train_end_day-WINDOW_SIZE+1)/train_batch_size,\n",
    "                              # validation_data=test_data_generator.generate(),\n",
    "                              # validation_steps=NUM_SAMPLES*(test_end_day-test_start_day-WINDOW_SIZE+1)/test_batch_size, \n",
    "                              callbacks=callbacks_list,\n",
    "                              epochs = num_epochs, verbose=1,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and history for future reuse\n",
    "model.save('./data/models/mlp_clusterbig.h5')\n",
    "with open('./data/models/mlp_history_clusterbig', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) Predict sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./data/models/mlp_clusterbig_best.h5') # LOAD THE MODEL YOU WANT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sales (day1 to last day):\n",
      "[0.0, 3.0, 0.0, 4.0, 2.0, 10.0, 8.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0]\n",
      "Actual sales (day1 to last day):\n",
      "[ 4.  7.  9.  5.  5.  2.  2.  4.  3.  9.  7.  5.  5.  5.  8.  2.  6.  6.\n",
      "  4.  2.  5. 55. 27. 44.  3.  0.  1.  2.  4. 20. 50.]\n"
     ]
    }
   ],
   "source": [
    "# For future predictions, we have to replace the actual past sales with the predicted sales \n",
    "sample_product = '19288L' # Change as required\n",
    "index = int((keys_dates.index[keys_dates['key'] == sample_product][0])/123) \n",
    "predictions = []\n",
    "product_data = np.zeros(shape=(1, 123, num_vars)) # A copy of the data for replacement\n",
    "product_data[0, :, :] = X[index*123:index*123+123]\n",
    "\n",
    "product_X = np.zeros(shape=(1, WINDOW_SIZE, num_vars)) # This will be pushed into model\n",
    "for i in range(31):\n",
    "    product_X[0, :, :] = product_data[0, 63+i:63+WINDOW_SIZE+i]\n",
    "    predictions.insert(0, round(model.predict(product_X)[0][0]))\n",
    "    # Update product data with predictions (we are not supposed to know the actual values)\n",
    "    if i == 0:\n",
    "        product_data[0, 93+i, 0] = predictions[0] # [93] = 2018-01-02 input\n",
    "    elif i < 28:\n",
    "        product_data[0, 93+i, 0:i+1] = predictions[0:i+1]\n",
    "    elif i < 30: # No need to update for i = 30 (last prediction day)\n",
    "        product_data[0, 93+i, 0:28] = predictions[0:28] # We keep taking the last 28 predictions\n",
    "predictions = predictions[::-1] # We have to reverse the list to get back original order\n",
    "print(\"Predicted sales (day1 to last day):\")\n",
    "print(predictions)\n",
    "print(\"Actual sales (day1 to last day):\")\n",
    "print(Y[index*123+92:index*123+123].reshape(31,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the sample output above. If predictions look odd, go back to 'Model definition' and 'Model training'. If predictions look ok, proceed to 'Predict all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) Predict all; takes about 6 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def pred_soldout_day(pred_sales, stock):\n",
    "    \"\"\"\n",
    "    Helper function to predict soldout day\n",
    "    \"\"\"\n",
    "    result = 15 if stock <= 3 else len(pred_sales) # If stock is too few, middle is a good guess\n",
    "    for day in range(len(pred_sales)):\n",
    "        stock -= pred_sales[day]\n",
    "        if stock <= 0:\n",
    "            return day+1\n",
    "    return result\n",
    "\n",
    "def load_test_set(set_number):\n",
    "    \"\"\"\n",
    "    Helper function to load and prepare test set\n",
    "    \"\"\"\n",
    "    test = pd.read_csv('./data/test/test_{}.csv'.format(str(set_number)), index_col=0, \n",
    "                       dtype={'pid': np.int32})\n",
    "    test['key'] = test['pid'].astype(str) + test['size']\n",
    "    test = test[test['key'].isin(big_key.astype(str))].reset_index(drop=True) # FOR BIG CLUSTER\n",
    "    print('Number of products in test data belonging to cluster: ' + str(len(test['key'].unique())))\n",
    "    \n",
    "    def extract_soldout_day(row):\n",
    "        soldout_day = row['sold_out_date'][-2:]\n",
    "        return int(soldout_day[1]) if soldout_day[0] == 0 else int(soldout_day)\n",
    "    \n",
    "    test['soldout_day'] = test.apply(extract_soldout_day, axis=1)\n",
    "    test['pred_soldout_day'] = np.nan\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2637\n",
      "Predictions complete\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for all products and store them\n",
    "test = load_test_set(0)\n",
    "all_predictions = pd.DataFrame()\n",
    "\n",
    "# We only make predictions for the products in the test set\n",
    "for row in range(test.shape[0]):\n",
    "    key = str(test.iloc[row]['key'])\n",
    "    \n",
    "    # Make predictions for current product\n",
    "    index = int(keys_dates[keys_dates['key'] == key].index[0]/123) # Get starting row of product\n",
    "    predictions = []\n",
    "    product_data = np.zeros(shape=(1, 123, num_vars)) # A copy of the data for replacement\n",
    "    product_data[0, :, :] = X[index*123:index*123+123]\n",
    "    \n",
    "    product_X = np.zeros(shape=(1, WINDOW_SIZE, num_vars)) # This will be pushed into model\n",
    "    for i in range(31):\n",
    "        product_X[0, :, :] = product_data[0, 63+i:63+WINDOW_SIZE+i]\n",
    "        predictions.insert(0, round(model.predict(product_X)[0][0]))\n",
    "        # Update product data with predictions (we are not supposed to know the actual values)\n",
    "        if i == 0:\n",
    "            product_data[0, 93+i, 0] = predictions[0] # [93] = 2018-01-02 input\n",
    "        elif i < 28:\n",
    "            product_data[0, 93+i, 0:i+1] = predictions[0:i+1]\n",
    "        elif i < 30: # No need to update for i = 30 (last prediction day)\n",
    "            product_data[0, 93+i, 0:28] = predictions[0:28] # Take only last 28 predictions\n",
    "    predictions = predictions[::-1] # Reverse the list to get back original order\n",
    "    all_predictions[key] = predictions\n",
    "\n",
    "all_predictions.to_csv('./data/test/pred_mlp_clusterbig.csv', index=False)\n",
    "print(\"Predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate on test data; takes about 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13. 16. 16.  7. 12. 10. 23. 27. 29. 35. 31. 31. 27. 22. 21. 20. 23. 23.\n",
      " 29. 34. 36. 36. 34. 30. 28. 26. 22. 20. 21. 25. 28.]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "prod_pred = all_predictions['12985L'].as_matrix()\n",
    "print(prod_pred)\n",
    "print(pred_soldout_day(prod_pred, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0: 155.33834040570923\n",
      "2637\n"
     ]
    }
   ],
   "source": [
    "# Go through all test sets\n",
    "for set_num in range(5):\n",
    "    test = load_test_set(set_num)\n",
    "    for row in range(test.shape[0]):\n",
    "        key = test['key'][row]\n",
    "        stock = test['stock'][row]\n",
    "        key_pred = all_predictions[key].as_matrix()\n",
    "        test['pred_soldout_day'][row] = pred_soldout_day(key_pred, stock)\n",
    "    test.to_csv('./data/test/mlp_clusterbig_{}_result.csv'.format(str(set_num))) # Dump test set result\n",
    "    print('Test {}: {}'.format(str(set_num), \n",
    "                               str(np.sqrt(sum(abs(test['pred_soldout_day'] - test['soldout_day']))))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
